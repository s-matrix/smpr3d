# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/10d_algorithm_beamlet_admm.ipynb (unless otherwise specified).


from __future__ import annotations


__all__ = ['fasta2', 'TorchTomographySolver', 'AETDataset', 'PhaseContrastScattering', 'bin_obj', 'complex_abs',
           'propagate_parallax_shift_Smatrix_kernel', 'propagate_parallax_shift_Smatrix_kernel',
           'propagate_parallax_shift_Smatrix', 'depth_section', 'ADMMOptions', 'SMPRSolution', 'gaussian', 'admm']

# Cell

import torch as th
from timeit import default_timer as timer
import logging
from torch.utils.tensorboard import SummaryWriter
from torch.nn import Module
from .util import *
from .data import SMeta
import numpy as np
from .operators import smatrix_phase_factorsBDK

def fasta2(s_meta: SMeta, A, AH_S, AH_Psi, AH_r, prox_data, Psi_gen: Module,
           a: th.Tensor, S0: th.Tensor, Psi0, C0: th.Tensor, Ap0: th.Tensor, r0: th.Tensor, opts,
           S_sol: th.Tensor = None, Psi_sol: th.Tensor = None, r_sol: th.Tensor = None,
           summary: SummaryWriter = None):
    """
    Block coordinate descent for scanning diffraction experiments with the latent variables
    S: represents the sample, Psi: represents the incoming wave function, r: represents the scanning positions

    :param A:        multi-linear forward operator. must be callable with 3 parameters: A(S, Psi, r)
    :param AH_S:     adjoint of the linear forward operator A_S with all variables fixed but S. Must be callable with 3 parameters: AH_S(z, Psi, r)
    :param AH_Psi:   adjoint of the linear forward operator A_Psi with all variables fixed but Psi. Must be callable with 3 parameters: AH_Psi(z, S, r)
    :param AH_r:     adjoint of the linear forward operator A_r with all variables fixed but r. Must be callable with 3 parameters: AH_r(z, S, Psi)
    :param prox_data:proximal operator for the data term
    :param Psi_gen:      a function that creates the Fourier space probe from aberration coefficients C
    :param a:        float (D, K, MY, MX) measured amplitudes
    :param S0:       float (B_tile, NY, NX, 2) initial S-matrix
    :param C0:       float (12, D) initial aberration coefficients
    :param A0:       float (D, MY, MX) initial apertures
    :param r0:       float (D, K, 2) initial positions
    :param opts:     dictionary of optimization options
    :param S_sol:    optional, float (B_tile, NY, NX, 2) initial S-matrix
    :param Psi_sol:  optional, float (D, MY, MX, 2) initial probes
    :param r_sol:    optional,float (D, K, 2) initial positions
    :param summary:  optional, tensorboard summary writer, used to write intermediate results to tensorboard for vizualisation
    :return: tuple (S, Psi, r), outs, opts
                outs is a dictionary with the keys "R_factors"
                    if a S_sol is provided it has a key "S_errors"
                    if a Psi_sol is provided it has a key "Psi_errors"
    """
    tau_S = opts.tau_S
    tau_Psi = opts.tau_Psi
    tau_r = opts.tau_r

    # lambda operator update speed
    beta = opts.beta
    device = S0.device

    # Maximum number of updates
    max_iters = opts.max_iters
    rank = opts.node_config.rank
    world_size = opts.node_config.world_size
    verbose = opts.verbose

    # Get diffraction pattern shape
    _, _, MY, MX = a.shape

    S_errors = []
    R_factors = []
    Psi_errors = []
    r_errors = []

    # Move probe, probe positions and scattering matrix to the chosen device
    Ap = Ap0.to(device)

    C = C0.to(device)
    C.requires_grad = True

    r = r0.to(device)
    S = S0.to(device)
    if Psi0 is not None:
        Psi = Psi0.to(device)
    else:
        Psi = Psi_gen(C, Ap)

    # use same phase factors for forward and backward
    Psi.phase_factors = smatrix_phase_factorsBDK(Psi, r, s_meta.take_beams, s_meta.q_dft, s_meta.S_shape[0], out=None)

    if rank == 0 and verbose >= 2:
        logging.info(f"a[0,0]**2 sum  : {(a[0, 0] ** 2).sum()}")
        # logging.info(f"z_hat[0,0] norm: {norm(z_hat[0, 0]) ** 2}")

    if world_size == 1:
        ii = 1
        # rexitw = th.ifft(z_hat[:, ii], 2, True)
        # plot_complex_multi(complex_numpy(rexitw.cpu()), f'real space exit waves, position {ii}')

    z1 = th.fft.fft2(A(S, Psi, r), norm='ortho')
    a_model = th.abs(z1)

    # cb = th.as_tensor(fftshift_checkerboard(MY//2,MX//2), device=z1.device)
    # ww = th.fft.fftshift(z1[0,:9],(1,2)) * cb
    #
    # plot_complex_multi(ww.cpu().numpy(), f'real space exit waves, position {ii}')

    # print(a.shape, a_model.shape)
    fac = (1 - (a / (a_model + 1e-3)))
    z1.mul_(fac)
    dz = th.fft.ifft2(z1, norm='ortho')

    S_mom = None
    C_mom = None
    m = [MY // 4, MX // 4]

    S_accel1 = S
    z_accel1 = z1
    alpha1 = 1
    momentum = 0.5
    dampening = 0

    for i in range(max_iters):
        start = timer()

        dS = AH_S(dz, Psi, r)

        if rank == 0 and i == 0:
            logging.info(f'max_abs_dS: {th.max(th.abs(dS))}   max_abs_S: {th.max(th.abs(S.detach()))}')
        if S_mom is None:
            S_mom = th.clone(dS).detach()
        else:
            S_mom.mul_(momentum).add_(dS, alpha=1 - dampening)

        dS.add_(S_mom, alpha=momentum)
        # plotAbsAngle(dS[0, m[0]:-m[0], m[1]:-m[1]].cpu().numpy(), f'dS[{i}]')

        S.add_(dS, alpha=-tau_S)
        del dS
        z1 = th.fft.fft2(A(S, Psi, r), norm='ortho')

        R_fac = R_factor(z1, a, world_size)

        a_model = th.abs(z1)
        fac = (1 - (a / (a_model + 1e-3)))
        z1.mul_(fac)
        dz = th.fft.ifft2(z1, norm='ortho')

        del a_model
        del fac
        del z1

        # solve for probe (step 3 of algorithm 1 from paper)
        if opts.optimize_psi(i):
            # Calculate probe update
            dPsi = AH_Psi(dz, S, r)
            if rank == 0 and i == 0:
                logging.info(f'max_abs_dPsi: {th.max(th.abs(dPsi))}   max_abs_Psi: {th.max(th.abs(Psi.detach()))}')
            # Psi -= tau_Psi * dPsi
            Psi = Psi_gen(C, Ap)
            Psi.backward(dPsi)
            dC = C.grad
            if C_mom is None:
                C_mom = th.clone(C.grad).detach()
            else:
                C_mom.mul_(momentum).add_(C.grad, alpha=1 - dampening)
            # Apply probe update
            dC = dC.add(C_mom, alpha=momentum)
            C = C.detach().clone().add(dC, alpha=- tau_Psi)
            C.requires_grad = True
            if rank == 0:
                logging.info(f'C[0] = {C[0].detach().cpu().numpy()}, C.grad = {dC[0].cpu().numpy()}')
            Psi = Psi_gen(C, Ap)
            Psi.phase_factors = smatrix_phase_factorsBDK(Psi, r, s_meta.take_beams, s_meta.qf, s_meta.q_coords, s_meta.S_shape[0], out=None)

            if verbose == 2 and rank == 0 and summary is not None:
                summary.add_scalar(f'details/max_abs_dPsi', th.max(th.abs(dPsi)), i, timer())
                summary.add_scalar(f'details/max_abs_Psi', th.max(th.abs(Psi.detach())), i, timer())
            del dPsi

        # Logging
        if verbose > 0:
            end = timer()
            R_factors.append(R_fac.cpu().item())
            s = f"{i:03d}/{max_iters:03d} [{(end - start):-02.4f}s] R-factor: {R_factors[-1]:3.3g}   "
            if summary is not None:
                if verbose >= 1 and rank == 0:
                    summary.add_scalar(f'errors/R_factor', R_fac, i, timer())
            if Psi_sol is not None:
                err_Psi = rel_dist(Psi.detach(), Psi_sol)
                Psi_errors.append(err_Psi.cpu().item())
                s += f"err_Psi: {Psi_errors[-1]:3.3g}   "
                if summary is not None and rank == 0:
                    if verbose >= 1:
                        summary.add_scalar(f'errors/probe_error', err_Psi, i, timer())
            if S_sol is not None and rank == 0:
                S[th.isnan(S)] = 0

                err_S = rel_dist(S[:, m[0]:-m[0], m[1]:-m[1]], S_sol[:, m[0]:-m[0], m[1]:-m[1]])
                # plotcxmosaic(complex_numpy(
                #     complex_mul_conj(S[:16, m[0]:-m[0], m[1]:-m[1]], S0[:16, m[0]:-m[0], m[1]:-m[1]]).cpu()),
                #              f'S_sol[{0}]')
                # plotcxmosaic(complex_numpy(S[:16, m[0]:-m[0], m[1]:-m[1]].cpu()), f'S[{i}]')
                # plotAbsAngle(complex_numpy(S[0, m[0]:-m[0], m[1]:-m[1]].cpu() - S_sol[0, m[0]:-m[0], m[1]:-m[1]].cpu()), 'S - S_sol')
                S_errors.append(err_S.cpu().item())
                s += f"err_S: {S_errors[-1]:3.3g}"
                if summary is not None:
                    if verbose >= 1:
                        summary.add_scalar(f'errors/S_error', err_S, i, timer())
            if rank == 0:
                logging.info(s)

    if rank == 0:
        logging.info(f'{i} iterations finished.')

    outs = Param()
    outs.R_factors = np.asarray(R_factors)
    if S_sol is not None:
        outs.S_errors = np.asarray(S_errors)
    if Psi_sol is not None:
        outs.Psi_errors = np.asarray(Psi_errors)
    if r_sol is not None:
        outs.r_errors = np.asarray(r_errors)
    S = S.cpu()
    Psi = Psi.detach().cpu()
    r = r.cpu().numpy()
    C = C.detach().cpu().numpy()

    return (S, Psi, C, r), outs, opts

# Cell
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
torch.set_printoptions(precision=10)

#data
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

import torch.optim as optim
import smpr3d.operators as op
import smpr3d.util as utilities
from .operators import Pupil
from .modules import SingleSlicePropagation, Defocus, MultislicePropagation
from .regularizers import Regularizer

import scipy.io as sio
import numpy as np

# Cell

bin_obj       = op.BinObject.apply
complex_abs   = op.ComplexAbs.apply

class TorchTomographySolver:
	def __init__(self, **kwargs):
		"""
		Creating tomography solver object.
		Required Args:
			shape: shape of the object in [y, x, z]
			voxel_size: size of voxel in [y, x, z]
			wavelength: wavelength of probing wave, scalar
			sigma: sigma used in calculating transmittance function (exp(1i * sigma * object)), scalar
			tilt_angles: an array of sample rotation angles
			defocus_list: an array of defocus values

		Optional Args [default]
			amplitude_measurements: measurements for reconstruction, not needed for forward evaluation of the model only [None]
			numerical_aperture: numerical aperture of the system, scalar [1.0]
			binning_factor: bins the number of slices together to save computation, scalar [1]
			pad_size: padding reconstruction from measurements in [dy,dx], final size will be measurement.shape + 2*[dy, dx], [0, 0]
			shuffle: random shuffle of measurements, boolean [True]
			pupil: inital value for the pupil function [None]
			maxitr: maximum number of iterations [100]
			step_size: step_size for each gradient update [0.1]
			momentum: [0.0 NOTIMPLEMENTED]

			-- transform alignment parameters (currently only support rigid body transform alignment) --
			transform_align: whether to turn on transform alignment, boolean, [False]
			ta_method: "turboreg"
			ta_start_iteration: alignment process will not start until then, int, [0]
			ta_iterations: iterations during which the alignment process will be on, [0, max_itr]

			-- Shift alignment parameters --
			shift_align: whether to turn on alignment, boolean, [False]
			sa_method: shift alignment method, can be "gradient", "hybrid_correlation", "cross_correlation", or "phase_correlation", string, ["gradient"]
			sa_step_size: step_size of shift parameters, float, [0.1]
			sa_start_iteration: alignment process will not start until then, int, [0]
			sa_iterations: iterations during which the alignment process will be on, [0, max_itr]

			-- Defocus refinement parameters --
			defocus_refine: whether to turn on defocus refinement for each measurement, boolean, [False]
			dr_method: defocus refinement method, can be "gradient", string, ["gradient"]
			dr_step_size: step_size of defocus refinement parameters, float, [0.1]
			dr_start_iteration: refinement process will not start until then, int, [0]
			dr_iterations: iterations during which the defocus refocus process will be on, [0, max_itr]

			-- regularizer parameters --
			regularizer_total_variation: boolean [False]
			regularizer_total_variation_gpu: boolean [False]
			regularizer_total_variation_parameter: controls amount of total variation, scalar or vector of length maxitr. [scalar 1.0]
			regularizer_total_variation_maxitr: number of iterations for total variation, integer [15]
			regularizer_total_variation_order: differential order, scalar [1], higher order not yet implemented
			regularizer_pure_real: boolean [False]
			regularizer_pure_imag: boolean [False]
			regularizer_pure_amplitude: boolean [False]
			regularizer_pure_phase: boolean [False]
			regularizer_positivity_real: boolean [False]
			regularizer_positivity_imag: boolean [False]
			regularizer_negativity_real: boolean [False]
			regularizer_negativity_imag: boolean [False]
			regularizer_dtype: torch dtype class [torch.float32]
		"""
		
		self.shape 			     = kwargs.get("shape")
		
		self.shuffle		     = kwargs.get("shuffle",              True)
		self.optim_max_itr       = kwargs.get("maxitr",               100)
		self.optim_step_size     = kwargs.get("step_size",            0.1)
		self.optim_momentum      = kwargs.get("momentum",             0.0)

		self.obj_update_iterations = kwargs.get("obj_update_iterations", np.arange(self.optim_max_itr))

		self.flag_gpu            = kwargs.get("flag_gpu",             True)
		self.device              = torch.device('cuda') if self.flag_gpu else torch.device('cpu')
		kwargs["device"]         = self.device
		#parameters for transform alignment
		self.transform_align     = kwargs.get("transform_align",      False)
		self.ta_method           = kwargs.get("ta_method",            "turboreg")
		self.ta_start_iteration  = kwargs.get("ta_start_iteration",   0)
		self.ta_iterations       = kwargs.get("ta_iterations",        None)
		if self.ta_iterations is None:
			self.ta_iterations = np.arange(self.ta_start_iteration, self.optim_max_itr)

		#parameters for shift alignment
		self.shift_align         = kwargs.get("shift_align",          False)
		self.sa_method           = kwargs.get("sa_method",            "gradient")
		self.sa_step_size        = kwargs.get("sa_step_size",         0.1)
		self.sa_start_iteration  = kwargs.get("sa_start_iteration",   0)
		self.sa_iterations       = kwargs.get("sa_iterations",        None)
		if self.sa_iterations is None:
			self.sa_iterations = np.arange(self.sa_start_iteration, self.optim_max_itr)

		#parameters for defocus refinement
		self.defocus_refine      = kwargs.get("defocus_refine",       False)
		self.dr_method           = kwargs.get("dr_method",            "gradient")
		self.dr_step_size        = kwargs.get("dr_step_size",         0.1)
		self.dr_start_iteration  = kwargs.get("dr_start_iteration",   0)		
		self.dr_iterations       = kwargs.get("dr_iterations",        None)
		if self.dr_iterations is None:
			self.dr_iterations = np.arange(self.dr_start_iteration, self.optim_max_itr)

		if not utilities.is_valid_method(self.sa_method):
			raise ValueError('Shift alignment method not valid.')
		if self.shift_align and utilities.is_correlation_method(self.sa_method):
			self.shift_obj		 = utilities.ImageShiftCorrelationBased(kwargs["amplitude_measurements"].shape[0:2], \
										    					    upsample_factor = 10, method = self.sa_method, \
											 					    device=torch.device('cpu'))

		if self.transform_align:
			self.transform_obj   = utilities.ImageTransformOpticalFlow(kwargs["amplitude_measurements"].shape[0:2],\
				         											   method = self.ta_method)

		self.dataset      	     = AETDataset(**kwargs)
		self.num_defocus	     = self.dataset.get_all_defocus_lists().shape[0]
		self.num_rotation        = len(self.dataset.tilt_angles)
		self.tomography_obj      = PhaseContrastScattering(**kwargs)
		reg_temp_param           = kwargs.get("regularizer_total_variation_parameter", None)
		if reg_temp_param is not None:
			if not np.isscalar(reg_temp_param):
				assert self.optim_max_itr == len(kwargs["regularizer_total_variation_parameter"])
		self.regularizer_obj     = Regularizer(**kwargs)
		self.rotation_obj	     = utilities.ImageRotation(self.shape, axis = 0, device=torch.device('cuda'))
		
		self.cost_function       = nn.MSELoss(reduction='sum')
    	
	def run(self, obj_init=None, forward_only=False, callback=None):
		"""
		run tomography solver
		Args:
		forward_only: True  -- only runs forward model on estimated object
					  False -- runs reconstruction
		"""
		if forward_only:
			self.shuffle = False
			amplitude_list = []
		
		self.dataloader = DataLoader(self.dataset, batch_size = 1, shuffle=self.shuffle)

		error = []
    	#initialize object
		self.obj = obj_init
		if self.obj is None:
			self.obj = op.r2c(torch.zeros(self.shape).to(self.device))
		else:
			if self.device == torch.device('cuda'):
				if not self.obj.is_cuda:
					self.obj = self.obj.to(self.device)
			self.obj = op.r2c(self.obj)
		
		#initialize shift parameters
		self.yx_shifts = None
		if self.shift_align:
			self.sa_pixel_count = []
			self.yx_shift_all = []
			self.yx_shifts = torch.zeros((2, self.num_defocus, self.num_rotation))

		if self.transform_align:
			self.xy_transform_all = []
			self.xy_transforms = torch.zeros((6, self.num_defocus, self.num_rotation))
#			self.xy_transforms = torch.zeros((3, self.num_defocus, self.num_rotation))
		# TEMPP
		# defocus_list_grad = torch.zeros((self.num_defocus, self.num_rotation), dtype = torch.float32)
		ref_rot_idx = None
		#begin iteration
		for itr_idx in range(self.optim_max_itr):
			sys.stdout.flush()
			running_cost = 0.0
			#defocus_list_grad[:] = 0.0
			if self.shift_align and itr_idx in self.sa_iterations:
				running_sa_pixel_count  = 0.0
			for data_idx, data in enumerate(self.dataloader, 0):
	    		#parse data
				if not forward_only:
					amplitudes, rotation_angle, defocus_list, rotation_idx = data
					if ref_rot_idx is None and abs(rotation_angle-0.0) < 1e-2:
						ref_rot_idx = rotation_idx
						print("reference index is:", ref_rot_idx)
					amplitudes = torch.squeeze(amplitudes)
					if len(amplitudes.shape) < 3:
						amplitudes = amplitudes.unsqueeze(-1)

				else:
					rotation_angle, defocus_list, rotation_idx = data[-3:]
				#prepare tilt specific parameters
				defocus_list = torch.flatten(defocus_list).to(self.device)
				rotation_angle = rotation_angle.item()
				yx_shift = None
				if self.shift_align and self.sa_method == "gradient" and itr_idx in self.sa_iterations:
					yx_shift = self.yx_shifts[:,:,rotation_idx]
					yx_shift = yx_shift.to(self.device)
					yx_shift.requires_grad_()
				if self.defocus_refine and self.dr_method == "gradient" and itr_idx in self.dr_iterations:
					defocus_list.requires_grad_()					
				#rotate object
				if data_idx == 0:
					self.obj = self.rotation_obj.forward(self.obj, rotation_angle)
				else:
					if abs(rotation_angle - previous_angle) > 90:
						self.obj = self.rotation_obj.forward(self.obj, -1 * previous_angle)
						self.obj = self.rotation_obj.forward(self.obj, rotation_angle)
					else:		
						self.obj = self.rotation_obj.forward(self.obj, rotation_angle - previous_angle)					
				if not forward_only:
					#define optimizer
					optimizer_params = []
					
					if itr_idx in self.obj_update_iterations:
						self.obj.requires_grad_()
						optimizer_params.append({'params': self.obj, 'lr': self.optim_step_size})
					if self.shift_align and self.sa_method == "gradient" and itr_idx in self.sa_iterations:
						optimizer_params.append({'params': yx_shift, 'lr': self.sa_step_size})
					if self.defocus_refine and self.dr_method == "gradient" and itr_idx in self.dr_iterations:
						optimizer_params.append({'params': defocus_list, 'lr': self.dr_step_size})
					optimizer = optim.SGD(optimizer_params)
				
				#forward scattering
				estimated_amplitudes = self.tomography_obj(self.obj, defocus_list, yx_shift)
				#in-plane rotation estimation
				if not forward_only:
					if self.transform_align and itr_idx in self.ta_iterations:
						if rotation_idx != ref_rot_idx:
							amplitudes, xy_transform = self.transform_obj.estimate(estimated_amplitudes, amplitudes)						
							xy_transform = xy_transform.unsqueeze(-1)
#						self.dataset.update_amplitudes(amplitudes, rotation_idx)
					#Correlation based shift estimation
					if self.shift_align and utilities.is_correlation_method(self.sa_method) and itr_idx in self.sa_iterations:
						if rotation_idx != ref_rot_idx:
							amplitudes, yx_shift, _ = self.shift_obj.estimate(estimated_amplitudes, amplitudes)
							yx_shift = yx_shift.unsqueeze(-1)
#						self.dataset.update_amplitudes(amplitudes, rotation_idx)
					if itr_idx == self.optim_max_itr - 1:
						print("Last iteration: updated amplitudes")
						self.dataset.update_amplitudes(amplitudes, rotation_idx)

			    		#compute cost
					cost = self.cost_function(estimated_amplitudes, amplitudes.to(self.device))
					running_cost += cost.item()

					#backpropagation
					cost.backward()
					#update object
					# if itr_idx >= self.dr_start_iteration:
					# 	# print(torch.norm(defocus_list.grad.data))
					# 	defocus_list_grad[:,data_idx] = defocus_list.grad.data *  self.dr_step_size
					optimizer.step()
					optimizer.zero_grad()
					del cost
				else:
					#store measurement
					amplitude_list.append(estimated_amplitudes.cpu().detach())
				del estimated_amplitudes
				self.obj.requires_grad = False
				if not forward_only:
					#keep track of shift alignment for the tilt
					if self.shift_align and itr_idx in self.sa_iterations:
						if yx_shift is not None:
							yx_shift.requires_grad = False
							if rotation_idx != ref_rot_idx:
								self.yx_shifts[:,:,rotation_idx] = yx_shift[:].cpu()
								running_sa_pixel_count += torch.sum(torch.abs(yx_shift.cpu().flatten()))
					
					#keep track of transform alignment for the tilt
					if self.transform_align and itr_idx in self.ta_iterations:
						if rotation_idx != ref_rot_idx:				
							self.xy_transforms[...,rotation_idx] = xy_transform[:].cpu()

					#keep track of defocus alignment for the tilt
					if self.defocus_refine and itr_idx in self.dr_iterations:
						defocus_list.requires_grad = False
						self.dataset.update_defocus_list(defocus_list[:].cpu().detach(), rotation_idx)

				previous_angle = rotation_angle
				
				#rotate object back
				if data_idx == (self.dataset.__len__() - 1):
					previous_angle = 0.0
					self.obj = self.rotation_obj.forward(self.obj, -1.0*rotation_angle)
				print("Rotation {:03d}/{:03d}.".format(data_idx+1, self.dataset.__len__()), end="\r")
			
			#apply regularization
			amplitudes = None
			if self.device == torch.device("cuda"):
				torch.cuda.empty_cache()
			if not forward_only:
				if itr_idx in self.obj_update_iterations:
					self.obj = self.regularizer_obj.apply(self.obj)
			error.append(running_cost)

			#keep track of shift alignment results
			if self.shift_align and itr_idx in self.sa_iterations:
				self.sa_pixel_count.append(running_sa_pixel_count)		
				self.yx_shift_all.append(np.array(self.yx_shifts).copy())
			
			#keep track of transform alignment results
			if self.transform_align and itr_idx in self.ta_iterations:
				self.xy_transform_all.append(np.array(self.xy_transforms).copy())

			if callback is not None:
				callback(self.obj.cpu().detach(), error)
				#TEMPPPPP
				# callback(defocus_list_grad, self.dataset.get_all_defocus_lists(), error)
			if forward_only and itr_idx == 0:
				return torch.cat([torch.unsqueeze(amplitude_list[idx],-1) for idx in range(len(amplitude_list))], axis=-1)
			print("Iteration {:03d}/{:03d}. Error: {:03f}".format(itr_idx+1, self.optim_max_itr, np.log10(running_cost)))

		self.defocus_list = self.dataset.get_all_defocus_lists()
		return self.obj.cpu().detach(), error

class AETDataset(Dataset):
	def __init__(self, amplitude_measurements=None, tilt_angles=[0], defocus_list=None, **kwargs):
		"""
		Args:
		    transform (callable, optional): Optional transform to be applied
		        on a sample.
		"""
		self.amplitude_measurements = amplitude_measurements
		if self.amplitude_measurements is not None:
			self.amplitude_measurements = amplitude_measurements.astype("float32")
		if tilt_angles is not None:
			self.tilt_angles = tilt_angles * 1.0
		if defocus_list is not None:
			if not torch.is_tensor(defocus_list):
				defocus_list = torch.tensor(defocus_list)
			if len(defocus_list.shape) == 1:
				self.defocus_list = defocus_list.unsqueeze(1).repeat(1, len(self.tilt_angles)) * 1.0
			elif len(defocus_list.shape) == 2:
				assert defocus_list.shape[1] == len(tilt_angles)
				self.defocus_list = defocus_list * 1.0
			else:
				raise ValueError('Invalid defocus_list shape.')

	def __len__(self):
		return self.tilt_angles.shape[0]

	def __getitem__(self, idx):
        #X x Y x #defocus
		if self.amplitude_measurements is not None:
			return self.amplitude_measurements[...,idx], self.tilt_angles[idx], self.defocus_list[:,idx], idx
		else:
			return self.tilt_angles[idx], self.defocus_list[:,idx], idx

	def update_defocus_list(self,defocus_list, idx):
		self.defocus_list[:,idx] = defocus_list.unsqueeze(-1)
		return

	def update_amplitudes(self, amplitudes, idx):
		self.amplitude_measurements[...,idx] = amplitudes
		return

	def get_all_defocus_lists(self):
		return self.defocus_list

	def get_all_measurements(self):
		return self.amplitude_measurements


class PhaseContrastScattering(nn.Module):

	def __init__(self, shape, voxel_size, wavelength, sigma=None, binning_factor=1, pad_size=[0,0], **kwargs):
		"""
		Phase contrast scattering model
		Starts from a plane wave, 3D object, and a list of defocus distance (in Angstrom).
		Computes intensity phase contrast image after electron scatters through the sample using multislice algorithm
		Required Args:
			shape: shape of the object in [y, x, z]
			voxel_size: size of voxel in [y, x, z]
			wavelength: wavelength of probing wave, scalar

		Optional Args [default]:
			sigma: sigma used in calculating transmittance function (exp(1i * sigma * object)), scalar [None]
			binning_factor: bins the number of slices together to save computation (loses accuracy), scalar [1]
			pad_size: padding reconstruction from measurements in [dy,dx], final size will be measurement.shape + 2*[dy, dx], [0, 0]
		"""
		super(PhaseContrastScattering, self).__init__()
		self.binning_factor = binning_factor
		self.shape          = shape
		self.pad_size       = pad_size
		self.voxel_size     = voxel_size
		self.wavelength     = wavelength
		
		#forward propagation
		self.shape_prop          = self.shape.copy()
		self.shape_prop[2]     //= self.binning_factor
		self.voxel_size_prop     = self.voxel_size.copy()
		self.voxel_size_prop[2] *= self.binning_factor
		self._propagation = MultislicePropagation(self.shape_prop, self.voxel_size_prop, self.wavelength, **kwargs)
		
		self.sigma          = sigma
		if self.sigma is None:
			self.sigma = (2 * np.pi / self.wavelength) * self.voxel_size_prop[2]

		#filter with aperture
		self._pupil = Pupil(self.shape[0:2], self.voxel_size[0], self.wavelength, **kwargs)

		#defocus operator
		self._defocus = Defocus(**kwargs)

		#shift correction
		self._shift = utilities.ImageShiftGradientBased(self.shape[0:2], **kwargs)

	def forward(self, obj, defocus_list, yx_shift=None):
		#bin object
		obj = bin_obj(obj, self.binning_factor)
		#raise to transmittance
		obj = torch.exp(1j * self.sigma * obj)
		#forward propagation & defocus add field_in
		field = self._propagation(obj)
		#pupil
		# field = self._pupil(field)
		#defocus		
		# field = field_defocus(field, self._propagation.propagate.kernel_phase, defocus_list)
		field = self._defocus(field, self._propagation.propagate.kernel_phase, defocus_list)
		#shift
		field = self._shift(field, yx_shift)
		#crop
		field = F.pad(field, (0,0, \
							  -1 * self.pad_size[1], -1 * self.pad_size[1], \
							  -1 * self.pad_size[0], -1 * self.pad_size[0]))
		#compute amplitude
		# amplitudes = complex_abs(field)

		return field








# Cell

import torch
torch.set_printoptions(precision=10)
import numba.cuda as cuda
import cmath as cm
import numpy as np
import torch as th
import math as m
from .util import *
from tqdm import tqdm
import scipy.io as sio
import numpy as np

# Cell

import math as m
import cmath as cm
from numba import cuda
@cuda.jit
def propagate_parallax_shift_Smatrix_kernel(S, lam, q, q2, beam_coords, t, out):
    n = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x
    B, NY, NX = S.shape
    N = B * NY * NX

    b = n // (NY * NX)
    ny = (n - b * (NY * NX)) // NX
    nx = (n - b * (NY * NX) - ny * NX)

    if n < N:
        dy = cm.tan(q[0, beam_coords[b, 0], beam_coords[b, 1]] * lam[0])
        dx = cm.tan(q[1, beam_coords[b, 0], beam_coords[b, 1]] * lam[0])
        phase = cm.pi * lam[0] * q2[ny, nx] + 2 * cm.pi * (q[0, ny, nx] * dy + q[1, ny, nx] * dx)
        val = cm.exp(1j * t[0] * phase)
        Sc = S[b, ny, nx]
        v = Sc * val
        # out[b, ny, nx, 0] = v.real
        # out[b, ny, nx, 1] = v.imag
        out[b, ny, nx] = v
@cuda.jit
def propagate_parallax_shift_Smatrix_kernel(S, lam, q, q2, beam_coords, t, out):
    n = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x
    B, NY, NX = S.shape
    N = B * NY * NX

    b = n // (NY * NX)
    ny = (n - b * (NY * NX)) // NX
    nx = (n - b * (NY * NX) - ny * NX)

    if n < N:
        dy = cm.tan(q[0, beam_coords[b, 0], beam_coords[b, 1]] * lam[0])
        dx = cm.tan(q[1, beam_coords[b, 0], beam_coords[b, 1]] * lam[0])
        phase = cm.pi * lam[0] * q2[ny, nx] + 2 * cm.pi * (q[0, ny, nx] * dy + q[1, ny, nx] * dx)
        val = cm.exp(1j * t[0] * phase)
        Sc = S[b, ny, nx]
        v = Sc * val
        # out[b, ny, nx, 0] = v.real
        # out[b, ny, nx, 1] = v.imag
        out[b, ny, nx] = v

def propagate_parallax_shift_Smatrix(S, lam, q, q2, beam_coords, t, out=None):
    if out is None:
        out = th.zeros_like(S)
    gpu = cuda.get_current_device()
    stream = th.cuda.current_stream().cuda_stream
    threadsperblock = gpu.MAX_THREADS_PER_BLOCK
    blockspergrid = m.ceil(np.prod(np.array(S.shape)) / threadsperblock)
    propagate_parallax_shift_Smatrix_kernel[blockspergrid, threadsperblock, stream](S, lam, q, q2, beam_coords, t, out)
    return out

import torch as th
from tqdm import tqdm
def depth_section(smpr_solution : SMPRSolution, wavelength : float, t : th.tensor):
    """
    Create a depth section from the S-Matrix by propagating each beam back in spack and interfering them.
    :param smpr_solution: SMPRSolution with a reconstructed (B, NY, NX) S-Matrix and SMeta metadata
    :param wavelength: float, wavelength in Angstrom
    :param t: (T,) 1D tensor of defocus values to create depth section images at
    :return: stack of complex exit waves at depths t
    """
    smeta = smpr_solution.s_matrix_meta
    wt = th.as_tensor(smeta.natural_neighbor_weights).cuda().float()
    s_full = []
    for i, wi in enumerate(tqdm(wt, desc="Interpolating full S-Matrix")):
        s_full_i = th.sum(wi[:, None, None] * smpr_solution.smatrix, 0)
        s_full.append(s_full_i.cpu().numpy())
    s_full = np.array(s_full)
    S = th.as_tensor(s_full).cuda()
    S1 = th.fft.fft2(S, norm='ortho')
    device = S1.device
    lam1 = th.tensor([wavelength], device=device)
    EW = th.zeros(len(t), S.shape[1], S.shape[2], dtype=S.dtype, device=th.device('cpu'))
    for i, T in enumerate(tqdm(t, desc="Optical sectioning")):
        tt = th.tensor([T], device=device)
        SS = propagate_parallax_shift_Smatrix(S1, lam1, smeta.q, smeta.q2, smeta.all_beams_coords, tt)
        EW[i] = th.fft.ifft2(th.sum(SS, axis=0), norm='ortho').cpu()
    return EW

# old code
# @cuda.jit
# def propagate_parallax_shift_Smatrix_kernel(S, lam, q, q2, beam_coords, t, out):
#     n = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x
#     B, NY, NX, c = S.shape
#     N = B * NY * NX
#
#     b = n // (NY * NX)
#     ny = (n - b * (NY * NX)) // NX
#     nx = (n - b * (NY * NX) - ny * NX)
#
#     if n < N:
#         dy = cm.tan(q[0, beam_coords[b, 0], beam_coords[b, 1]] * lam[0])
#         dx = cm.tan(q[1, beam_coords[b, 0], beam_coords[b, 1]] * lam[0])
#         phase = cm.pi * lam[0] * q2[ny, nx] + 2 * cm.pi * (q[0, ny, nx] * dy + q[0, ny, nx] * dx)
#         val = cm.exp(1j * t[0] * phase)
#         Sc = S[b, ny, nx, 0] + 1j * S[b, ny, nx, 1]
#         v = Sc * val
#         out[b, ny, nx, 0] = v.real
#         out[b, ny, nx, 1] = v.imag
#
# def propagate_parallax_shift_Smatrix(S, lam, q, q2, beam_coords, t, out=None):
#     if out is None:
#         out = th.zeros_like(S)
#     gpu = cuda.get_current_device()
#     stream = th.cuda.current_stream().cuda_stream
#     threadsperblock = gpu.MAX_THREADS_PER_BLOCK
#     blockspergrid = m.ceil(np.prod(np.array(S.shape[:-1])) / threadsperblock)
#     propagate_parallax_shift_Smatrix_kernel[blockspergrid, threadsperblock, stream](S, lam, q, q2, beam_coords, t, out)
#     return out
#
# def depth_section(S, beam_indices, beam_coords, lam, dx, output_beam_mask, beam_block_mask, t):
#     """
#
#     :param S:
#     :param beam_indices: (B,) integer indices of beams
#     :param beam_coords: (B, 2) integer coordinates of beams
#     :param lam: wavelength
#     :param dx: real-space mapling of S-matrix
#     :param output_beam_mask: mask applied after shifting S-matrix to zero
#     :param beam_block_mask: mask applied after summing S-matrix
#     :param t: (n_depths,) defocus in Angstrom
#     :return:
#     """
#     device = S.device
#     dtype = S.dtype
#
#     # Get array shape of scattering matrix
#     B, Y, X = S.shape[:-1]
#     S1 = th.fft(S, 2, True)
#     q = fourier_coordinates_2D([Y, X], dx, centered=False)
#     q = th.from_numpy(q).type(dtype).to(device)
#     q2 = q[0] ** 2 + q[1] ** 2
#     # Shift all beams to origin for each component of the scattering matrix / remove beam tilts
#     mask = output_beam_mask.to(device)
#     mask2 = beam_block_mask.unsqueeze(0).unsqueeze(-1).to(device)
#     for ib, beam in zip(beam_indices, beam_coords):
#         by, bx = [x.item() for x in beam]
#         mask_shift = th.roll(mask, [-by, -bx], (0, 1))
#         S1[ib] = th.roll(S1[ib], [-by, -bx], [0, 1]) * mask_shift[..., None]
#
#     beam_coords1 = th.as_tensor(beam_coords, device=device)
#     lam1 = th.tensor([lam], device=device)
#
#     EW = th.zeros(len(t), Y, X, dtype=dtype, device=th.device('cpu'))
#     for i, T in enumerate(tqdm(t, desc="Optical section")):
#         tt = th.tensor([T], device=device)
#         SS = propagate_parallax_shift_Smatrix(S1, lam1, q, q2, beam_coords1, tt)
#         EW[i] = th.ifft(th.sum(SS * mask2.expand_as(SS), axis=0), 2, True).cpu()
#
#     return EW

# Cell
import numpy as np
import torch as th

#nbdev_comment from __future__ import annotations

from .operators import Qoverlap_real,  calc_psi_denom, \
    calc_psi, A_realspace, Qsplit, Qoverlap, SubpixShift, gradient_fourier_2d, prox_D_gaussian

from tqdm import trange
from .util import plotAbsAngle, plot
from typing import TYPE_CHECKING, List, NamedTuple, Optional, Union, Callable
from dataclasses import dataclass
from .data import LinearIndexEncoded4DDataset, Dense4DDataset, SMeta
from torch.utils.data import BatchSampler, SequentialSampler
import logging

@dataclass
class ADMMOptions:
    r12 : float
    r22 : float
    eps1 : float
    eps2 : float
    dL_dr_momentum : float
    dL_dr_step : float
    beta : float
    do_position_correction : Callable
    do_subpix : bool
    non_blocking : bool
    margin : int
    do_smoothing : bool
    kernel_size : tuple
    sigma : np.array

    def __init__(self,
                 beta = 0.9,
                 r12 = 1e-6,
                 r22 = 1e-3,
                 eps1 = 1e-6,
                 eps2 = 1e-6,
                 dL_dr_momentum = 1.0,
                 dL_dr_step = 1e-3,
                 do_position_correction = lambda it: True if it > 1 else False,
                 do_subpix = True,
                 non_blocking = False,
                 verbose = False,
                 do_smoothing = True,
                 margin = 5,
                 kernel_size = (9, 9),
                 sigma = np.array((2.5, 2.5)),
                 ):
        self.r12 = r12
        self.r22 = r22
        self.eps1 = eps1
        self.eps2 = eps2
        self.dL_dr_momentum = dL_dr_momentum
        self.dL_dr_step = dL_dr_step
        self.do_position_correction = do_position_correction
        self.do_subpix = do_subpix
        self.non_blocking = non_blocking
        self.beta = beta
        self.verbose = verbose
        self.do_smoothing = do_smoothing
        self.margin = margin
        self.kernel_size = kernel_size
        self.sigma = sigma

import h5py
@dataclass
class SMPRSolution:
    converged: bool
    smatrix: th.Tensor
    probe: th.Tensor
    positions: th.Tensor
    r_factor: float
    r_factor_history: th.Tensor
    s_matrix_meta : SMeta

    def __init__(self,
                 converged : bool,
                 smatrix : th.Tensor,
                 probe : th.tensor,
                 positions : th.Tensor,
                 r_factor : float,
                 r_factor_history : th.Tensor,
                 s_matrix_meta : SMeta):
        self.converged = converged
        self.smatrix = smatrix
        self.probe = probe
        self.positions = positions
        self.r_factor = r_factor
        self.r_factor_history = r_factor_history
        self.s_matrix_meta = s_matrix_meta

    def to_h5(self, file_path, key):
        with h5py.File(file_path, 'a') as f:
            g = f.create_group(key)
            g.create_dataset('converged', data=self.converged)
            g.create_dataset('smatrix', data=self.smatrix.cpu().numpy())
            g.create_dataset('probe', data=self.probe.cpu().numpy())
            g.create_dataset('positions', data=self.positions.cpu().numpy())
            g.create_dataset('r_factor', data=self.r_factor)
            g.create_dataset('r_factor_history', data=self.r_factor_history.cpu().numpy())
        self.s_matrix_meta.to_h5(file_path, key + 's_meta')

    @staticmethod
    def from_h5(file_path, key):
        with h5py.File(file_path, 'r') as f:
            g = f[key]
            converged = g['converged'][()]
            smatrix = g['smatrix'][...]
            probe = g['probe'][...]
            positions = g['positions'][...]
            r_factor = g['r_factor'][()]
            r_factor_history = g['r_factor_history'][...]

        s_meta = SMeta.from_h5(file_path, key + 's_meta')

        res = SMPRSolution(converged,
                           smatrix,
                           probe,
                           positions,
                           r_factor,
                           r_factor_history,
                           s_meta)
        return res

from kornia.filters import  gaussian_blur2d
def gaussian(x, kernel_size, sigma):
        srmax = x.real.max()
        simax = x.real.max()
        smr = gaussian_blur2d(x.real.unsqueeze(0), kernel_size, sigma,border_type='reflect')
        smi = gaussian_blur2d(x.imag.unsqueeze(0), kernel_size, sigma,border_type='reflect')
        smr = smr / smr.max() * srmax
        smi = smi / smr.max() * simax
        ret = smr + 1j * smi
        return th.clone(ret[0])

from timeit import default_timer as timer
def admm(measurements : Union[LinearIndexEncoded4DDataset, Dense4DDataset],
         r : th.tensor,
         psi0 : th.tensor,
         s_meta : SMeta,
         n_iter : int,
         n_batches : int,
         options : ADMMOptions) -> SMPRSolution:

    do_position_correction = options.do_position_correction
    do_subpix = options.do_subpix
    beta = options.beta
    dL_dr_momentum = options.dL_dr_momentum
    dL_dr_step = options.dL_dr_step
    non_blocking = options.non_blocking
    r12 = options.r12
    r22 = options.r22
    eps1 = options.eps1
    eps2 = options.eps2
    verbose = options.verbose
    margin = options.margin
    kernel_size = options.kernel_size
    sigma = options.sigma.copy()

    cx_dtype = th.complex64
    dev_z = th.device(f'cpu')
    dev_compute = [th.device(f'cuda:{i}') for i in [0]]

    M = s_meta.M
    MY, MX = M[0].item(), M[1].item()
    N = s_meta.N
    NY, NX = N[0].item(), N[1].item()
    K = r.shape[0]
    batch_size = int(np.ceil(K / n_batches))
    slic = np.s_[0, MY // 2 + margin:-MY *2 - margin, MX // 2 + margin:-MX *2 - margin]

    shift = SubpixShift(MY, MX, dev_compute[0])
    sampler = BatchSampler(SequentialSampler(range(K)), batch_size=batch_size, drop_last=False)

    if isinstance(measurements, Dense4DDataset):
        sum_I = 0
        for batch_inds in sampler:
            I_b = measurements[batch_inds]
            sum_I += th.sum(I_b).item()
    elif isinstance(measurements, LinearIndexEncoded4DDataset):
        sum_I = 0
        for batch_inds in sampler:
            I_b = measurements[batch_inds]
            sum_I += th.sum(I_b.counts).item()

    a_norm = np.sqrt(sum_I)
    Bp = s_meta.Bp

    z = th.zeros((K, MY, MX), dtype=cx_dtype, device=dev_z).pin_memory()
    S_model = th.zeros((Bp, NY, NX), dtype=cx_dtype, device=dev_compute[0])
    # S_model = th.ones((Bp, NY, NX), dtype=cx_dtype, device=dev_compute[0])
    # S_model.imag[:] = 0
    AtA = th.zeros((Bp, NY, NX), dtype=th.float32, device=dev_compute[0]) + 1e-6
    z_hat = th.zeros_like(z)
    Lambda = th.zeros_like(z)
    r = th.as_tensor(r, device=dev_compute[0])
    dL_dr_old = th.zeros_like(r)

    r_int = th.round(r).long()
    dr = r - r_int

    sampler = BatchSampler(SequentialSampler(range(K)), batch_size=batch_size, drop_last=False)
    # data_loader = DataLoader(measurements, shuffle=False, num_workers=0, pin_memory=False, sampler=sampler)

    for batch_inds in sampler:
        zb = z[batch_inds].to(dev_compute[0], non_blocking=non_blocking)
        I_b = measurements[batch_inds].to(dev_compute[0], non_blocking=non_blocking)
        K_b = zb.shape[0]

        psi = th.broadcast_to(psi0[:, None, ...], (Bp, K_b, MY, MX))
        zhb = th.fft.fft2(th.sum(psi, 0), norm='ortho') + 1e-4

        zb, _ = prox_D_gaussian(zb, zhb, I_b, 0)
        z[batch_inds] = zb.to(dev_z, non_blocking=non_blocking)

    for batch_inds in sampler:
        psi = shift(psi0, dr[batch_inds])
        AtA = Qoverlap_real(r_int[batch_inds], th.abs(psi) ** 2, AtA)

    for batch_inds in sampler:
        psi = shift(psi0, dr[batch_inds])
        zb = z[batch_inds].to(dev_compute[0], non_blocking=non_blocking)
        zb = th.fft.ifft2(zb, norm='ortho')
        zb = th.conj(psi) * zb
        zb = shift(zb, dr[batch_inds])
        S_model = Qoverlap(r_int[batch_inds], zb, S_model)
    S_model /= AtA

    if verbose:
        plotAbsAngle(S_model[slic].cpu(), 'S_model init')

    R_factors = []
    for i in trange(n_iter, desc = 'ADMM iterations'):
        start = timer()
        for batch_inds in sampler:
            zz = z[batch_inds].to(dev_compute[0], non_blocking=non_blocking)
            LL = Lambda[batch_inds].to(dev_compute[0], non_blocking=non_blocking)
            z_hat[batch_inds] = th.fft.ifft2(zz + LL / beta, norm='ortho').to(dev_z, non_blocking=non_blocking)

        if do_subpix and do_position_correction(i):
            for batch_inds in sampler:
                K_b = len(batch_inds)
                S_split = th.zeros((Bp, K_b, MY, MX), dtype=cx_dtype, device=dev_compute[0])
                z_hatb = z_hat[batch_inds].to(dev_compute[0], non_blocking=non_blocking)

                psi = shift(psi0, dr[batch_inds])
                S_split = Qsplit(r_int[batch_inds], S_model, S_split)

                # 2 x Bp x K x MY x MX
                dS_split_dr = gradient_fourier_2d(S_split)
                # 2 x K x Bp x MY x MX
                dS_split_drP = dS_split_dr * psi[None, ...]

                # 2 x Bp x K x MY x MX
                nom = th.real(th.conj(dS_split_drP) * z_hatb[None, None, ...])
                denom = th.abs(dS_split_drP) ** 2

                # 2 x K
                dL_dr = th.sum(nom, (1, 3, 4)) / th.sum(denom, (1, 3, 4))
                # K x 2
                dL_dr = dL_dr.transpose(0, 1)
                # max shift of +/-0.2 pixels
                dL_dr = th.min(th.stack([th.abs(dL_dr), th.zeros_like(dL_dr).fill_(0.2)]), 0).values * th.sgn(dL_dr)

                dL_dr_up = dL_dr * dL_dr_step + dL_dr_old[batch_inds] * dL_dr_momentum
                r[batch_inds] += dL_dr_up
                r[batch_inds, 0] = th.clamp(r[batch_inds, 0], 0, NY - MY)
                r[batch_inds, 1] = th.clamp(r[batch_inds, 1], 0, NX - MX)

                dL_dr_old[batch_inds] = dL_dr_up

        new_psi = th.zeros_like(psi0)
        new_psi_denom = th.zeros(psi0.shape, device=dev_compute[0])

        for batch_inds in sampler:
            K_b = len(batch_inds)
            S_split = th.zeros((Bp, K_b, MY, MX), dtype=cx_dtype, device=dev_compute[0])
            if do_subpix:
                z_hatb = z_hat[batch_inds].to(dev_compute[0], non_blocking=non_blocking)
                S_split[:] = 0
                S_split = Qsplit(r_int[batch_inds], S_model, S_split)

                psi0 = th.conj(S_split) * z_hatb
                # shift update in opposite direction
                psi0 = shift(psi0, -dr[batch_inds])
                # sum over K
                new_psi += th.sum(psi0, 1)

                S_split = shift(S_split, -dr[batch_inds])
                # sum over K

                new_psi_denom += th.sum(th.abs(S_split) ** 2, 1)
            else:
                new_psi_denom += calc_psi_denom(r_int[batch_inds], S_model, th.zeros(psi0.shape, device=dev_compute[0]))
                new_psi += calc_psi(r_int[batch_inds], S_model, z_hat[batch_inds], th.zeros_like(psi0))
        # # Bp x MY x MX
        psi0 = new_psi / new_psi_denom
        Psi0 = s_meta.beamlets * th.fft.fft2(psi0, norm='ortho')
        psi0 = th.fft.ifft2(Psi0, norm='ortho')
        del Psi0
        del new_psi
        del new_psi_denom
        del S_split

        # print(f"psi norm: {th.norm(psi0[0, 0])}")
        # plot(new_psi_denom[0].cpu().numpy(), 'psi_denom')
        # plotAbsAngle(psi0[0].cpu().numpy(),'torch')
        # plotcx(psi0[0].cpu().numpy(), 'torch')
        # update normalisation
        AtA[:] = 1e-6
        for batch_inds in sampler:
            psi = shift(psi0, dr[batch_inds])
            AtA = Qoverlap_real(r_int[batch_inds], th.abs(psi) ** 2, AtA)

        h2 = th.max(AtA)
        M2 = (h2 <= eps2) * eps2 + (h2 > eps2) * h2 * r22
        M2 = M2.to(th.float32)

        S_model_new = th.zeros_like(S_model)
        for batch_inds in sampler:
            psi = shift(psi0, dr[batch_inds])
            zhb = z_hat[batch_inds].to(dev_compute[0], non_blocking=non_blocking)
            S_model_new = Qoverlap(r_int[batch_inds], th.conj(psi) * zhb, S_model_new)

        S_model_new += M2 * S_model
        S_model = S_model_new / (AtA + M2)
        # del S_model_new

        if Bp > 1:
            S_model = gaussian(S_model, kernel_size, tuple(sigma))
            sigma *= 0.97

        # if verbose:
        #     plotAbsAngle(S_model[slic].cpu(), f'S_model {i}')
        # update model exit waves
        for batch_inds in sampler:
            psi = shift(psi0, dr[batch_inds])
            zh = A_realspace(r_int[batch_inds], S_model, psi, th.zeros_like(z_hat[batch_inds], device=dev_compute[0]))
            z_hat[batch_inds] = th.fft.fft2(zh, norm='ortho').to(dev_z, non_blocking=non_blocking)
        losses = []
        #  update model from data, update auxiliary variables
        for batch_inds in sampler:
            zhb = z_hat[batch_inds].to(dev_compute[0], non_blocking=non_blocking)
            Lb = Lambda[batch_inds].to(dev_compute[0], non_blocking=non_blocking)
            zb = z[batch_inds].to(dev_compute[0], non_blocking=non_blocking)
            I_b = measurements[batch_inds].to(dev_compute[0], non_blocking=non_blocking)

            zhb -= Lb / beta
            # zb, loss = sparse_amplitude_prox(zb, zhb, Ii_b, I_b, beta)
            zb, loss = prox_D_gaussian(zb, zhb, I_b, beta)
            zhb += Lb / beta

            Lb += beta * zb
            Lb -= beta * zhb

            z[batch_inds] = zb.to(dev_z, non_blocking=non_blocking)
            z_hat[batch_inds] = zhb.to(dev_z, non_blocking=non_blocking)
            Lambda[batch_inds] = Lb.to(dev_z, non_blocking=non_blocking)
            losses.append(loss)

        losses = np.concatenate(losses)
        R_factor = np.sqrt(np.sum(losses)) / a_norm
        R_factors.append(R_factor)
        end = timer()
        if options.verbose:
            logging.info(f"{i:03d}/{n_iter:03d} [{(end - start):-02.2f}s] R-factor: {R_factor:3.3g}")

        if do_position_correction(i):
            for batch_inds in sampler:
                r_int[batch_inds] = th.round(r[batch_inds]).long()
                dr[batch_inds] = r[batch_inds] - r_int[batch_inds]

    out = SMPRSolution(
        converged=True,
        smatrix = S_model,
        probe = psi0,
        positions = r,
        r_factor = R_factors[-1],
        r_factor_history = th.tensor(R_factors),
        s_matrix_meta = s_meta
    )

    return out