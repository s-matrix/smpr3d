{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from ncempy.io.dm import fileDM\n",
    "from smpr3d.util import wavelength\n",
    "from torch.utils.data import Dataset\n",
    "from skimage.filters import gaussian\n",
    "from smpr3d.util import crop_symmetric_around_center, sparse_to_dense_datacube_crop, \\\n",
    "    sparse_to_dense_datacube_crop_gain_mask, center_of_mass_kernel, sum_frames, rotate, virtual_annular_image_kernel, \\\n",
    "    fftshift_kernel, fftshift_pad_kernel, dense_to_sparse_kernel, fourier_coordinates_2D, \\\n",
    "    array_split_divpoints_ntotal, beamlet_samples, natural_neighbor_weights\n",
    "\n",
    "from __future__ import annotations\n",
    "import sigpy as sp\n",
    "from numba import cuda\n",
    "from fastcore.utils import basic_repr\n",
    "import h5py \n",
    "from typing import TYPE_CHECKING, List, NamedTuple, Optional, Union, Callable\n",
    "import torch as th\n",
    "import numpy as np \n",
    "from numpy.fft import fftshift\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "@dataclass\n",
    "class DetectorMeta:\n",
    "    bright_field_center : np.array\n",
    "    bright_field_radius : float    \n",
    "    \n",
    "    def __init__(self, bright_field_center, bright_field_radius):\n",
    "        self.bright_field_radius = bright_field_radius\n",
    "        self.bright_field_center = bright_field_center\n",
    "        \n",
    "    def to_h5(self, file_path, key = 'detector_meta'):\n",
    "            with h5py.File(file_path, 'a') as f:\n",
    "                g = f.create_group(key)\n",
    "                g.create_dataset('bright_field_radius', data=self.bright_field_radius)\n",
    "                g.create_dataset('bright_field_center', data=self.bright_field_center)\n",
    "            \n",
    "    @staticmethod        \n",
    "    def from_h5(file_path, key = 'detector_meta'):\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            g = f[key]\n",
    "            bright_field_center = g['bright_field_center'][...]\n",
    "            bright_field_radius = g['bright_field_radius'][()]\n",
    "        res = DetectorMeta(bright_field_center,bright_field_radius)\n",
    "        return res \n",
    "\n",
    "@dataclass\n",
    "class SMeta:\n",
    "    f : np.array\n",
    "    M : np.array\n",
    "    N : np.array\n",
    "    dx : np.array\n",
    "    \n",
    "    q : th.tensor\n",
    "    qf : th.tensor \n",
    "    q2 : th.tensor \n",
    "    qf2 : th.tensor \n",
    "    q_coords : th.tensor\n",
    "    r_indices : th.tensor\n",
    "    all_beams : th.tensor\n",
    "    parent_beams : th.tensor\n",
    "    beam_numbers : th.tensor\n",
    "    all_beams_q : th.tensor\n",
    "    all_beams_coords : th.tensor\n",
    "    parent_beams_q : th.tensor\n",
    "    parent_beams_coords : th.tensor\n",
    "    # natural_neighbor_weights : th.tensor\n",
    "    # beamlets : th.tensor\n",
    "    q_dft : th.tensor\n",
    "    \n",
    "    numerical_aperture_radius_pixels : int\n",
    "    B : int \n",
    "    Bp : int\n",
    "    \n",
    "    device : th.device\n",
    "    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 all_beams : np.array,                 \n",
    "                 parent_beams : np.array,\n",
    "                 numerical_aperture_radius_pixels,\n",
    "                 dx : np.array, \n",
    "                 N : tuple, \n",
    "                 M : tuple, \n",
    "                 device : th.device):\n",
    "        self.dx = dx\n",
    "        self.device = device\n",
    "        self.all_beams = th.as_tensor(all_beams).to(device).bool()\n",
    "        self.parent_beams = th.as_tensor(parent_beams).to(device).bool()\n",
    "        self.numerical_aperture_radius_pixels = numerical_aperture_radius_pixels\n",
    "        self.B = int(all_beams.sum())\n",
    "        self.Bp = int(parent_beams.sum())\n",
    "        \n",
    "        MY, MX = M\n",
    "        NY, NX = N\n",
    "        self.M = np.array(M)\n",
    "        self.N = np.array(N)\n",
    "\n",
    "        self.f = np.array([NY, NX]) // np.array([MY, MX])\n",
    "        self.q = th.as_tensor(fourier_coordinates_2D([NY, NX], dx, centered=False), device=device)\n",
    "        self.qf = th.as_tensor(fourier_coordinates_2D([MY, MX], dx, centered=False), device=device)\n",
    "        self.q2 = th.norm(self.q, dim=0) ** 2\n",
    "        self.qf2 = th.norm(self.qf, dim=0) ** 2\n",
    "        mgrid = np.array(np.mgrid[-MY // 2:MY // 2, -MX // 2:MX // 2])\n",
    "        self.q_coords = th.from_numpy(fftshift(mgrid, (1, 2))).to(device)\n",
    "        self.r_indices = th.from_numpy(np.mgrid[:MY, :MX]).to(device)\n",
    "        \n",
    "        self.beam_numbers = th.ones_like(self.all_beams, dtype=th.long, device=device) * -1\n",
    "        self.beam_numbers[self.all_beams] = th.arange(0, self.B, device=device)\n",
    "        \n",
    "        all_beams_expanded = self.all_beams[None,...].expand_as(self.q_coords)\n",
    "        self.all_beams_q = self.qf[all_beams_expanded].reshape(2,self.B).T\n",
    "        self.all_beams_coords = self.q_coords[all_beams_expanded].reshape(2,self.B).T\n",
    "        \n",
    "        parent_beams_expanded = self.parent_beams[None,...].expand_as(self.q_coords)\n",
    "        self.parent_beams_q = self.qf[parent_beams_expanded].reshape(2,self.Bp).T\n",
    "        self.parent_beams_coords = self.q_coords[parent_beams_expanded].reshape(2,self.Bp).T\n",
    "        \n",
    "        self.q_dft = th.from_numpy(fourier_coordinates_2D([MY, MX], [1, 1], centered=False)).to(device)\n",
    "        \n",
    "        #S_shape          array (3,)        \n",
    "        #q              (NY, NX) x\n",
    "        #qf             (MY, MX) x\n",
    "        #q2             (NY, NX) x\n",
    "        #q2f            (MY, MX) x\n",
    "        #f              (2,)\n",
    "        #q_coords       (MY, MX)\n",
    "        #r_indices      (NY, NX)\n",
    "        #all_beams     (MY, MX)\n",
    "        #beam_numbers   (MY, MX)\n",
    "        #q_b            (B, 2)\n",
    "        #q_b_coords     (B, 2)\n",
    "        #q_dft          (MY, MX)\n",
    "        \n",
    "    def to_h5(self, file_path, key = 's_matrix_meta'):\n",
    "        with h5py.File(file_path, 'a') as f:\n",
    "            g = f.create_group(key)\n",
    "            g.create_dataset('M', data=self.M)\n",
    "            g.create_dataset('N', data=self.N)\n",
    "            g.create_dataset('all_beams', data=self.all_beams.cpu().numpy())\n",
    "            g.create_dataset('parent_beams', data=self.parent_beams.cpu().numpy())\n",
    "            g.create_dataset('numerical_aperture_radius_pixels', data=self.numerical_aperture_radius_pixels)\n",
    "            g.create_dataset('dx', data=self.dx)\n",
    "            g.create_dataset('device', data=str(self.device))\n",
    "            \n",
    "    @staticmethod        \n",
    "    def from_h5(file_path, key = 's_matrix_meta'):\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            g = f[key]\n",
    "            all_beams = g['all_beams'][...]\n",
    "            parent_beams = g['parent_beams'][...]\n",
    "            numerical_aperture_radius_pixels = g['numerical_aperture_radius_pixels'][()]\n",
    "            dx = g['dx'][...]\n",
    "            N = g['N'][...]\n",
    "            M = g['M'][...]\n",
    "            device = th.device(str(g['device'][...]))\n",
    "        res = SMeta(all_beams, parent_beams, numerical_aperture_radius_pixels, dx, N, M, device)\n",
    "        return res \n",
    "    \n",
    "    def make_beamlet_meta(self, n_radial_samples, n_angular_samples = 6) -> SMeta:\n",
    "        parent_beams_coords = beamlet_samples(self.all_beams.cpu().numpy(), \n",
    "                            self.numerical_aperture_radius_pixels,\n",
    "                            n_angular_samples, \n",
    "                            n_radial_samples) \n",
    "        \n",
    "        parent_beams = th.zeros(tuple(self.M), dtype=th.bool)\n",
    "        for si in parent_beams_coords:\n",
    "            parent_beams[si[0],si[1]] = 1\n",
    "            \n",
    "        Bp = parent_beams_coords.shape[0]\n",
    "        mgrid = np.array(np.mgrid[-self.M[0] // 2:self.M[0] // 2, -self.M[1] // 2:self.M[1] // 2])\n",
    "        q_coords = th.from_numpy(fftshift(mgrid, (1, 2)))\n",
    "        parent_beams_expanded = parent_beams[None,...].expand_as(q_coords)\n",
    "        parent_beams_coords = q_coords[parent_beams_expanded].reshape(2,Bp).T\n",
    "            \n",
    "        if n_radial_samples > 1:\n",
    "            nnw = natural_neighbor_weights(parent_beams_coords, \n",
    "                                          self.all_beams_coords.cpu().numpy(), \n",
    "                                          minimum_weight_cutoff=1e-2)\n",
    "        else:\n",
    "            nnw = np.ones((self.B, 1))\n",
    "            \n",
    "        beamlets = []\n",
    "        for j in range(nnw.shape[1]):\n",
    "            wsample = nnw[:, j]\n",
    "            ww = np.zeros(self.all_beams.shape, dtype=np.float32)\n",
    "            ww[self.all_beams_coords[:, 1].cpu().numpy(), \n",
    "               self.all_beams_coords[:, 0].cpu().numpy()] = wsample\n",
    "            beamlets.append(ww)\n",
    "        beamlets = np.array(beamlets)\n",
    "            \n",
    "        new_meta = SMeta(self.all_beams, \n",
    "                         parent_beams, \n",
    "                         self.numerical_aperture_radius_pixels, \n",
    "                         self.dx, \n",
    "                         tuple(self.N), \n",
    "                         tuple(self.M), \n",
    "                         self.device\n",
    "                         )\n",
    "        new_meta.natural_neighbor_weights = th.as_tensor(nnw)\n",
    "        new_meta.beamlets = th.as_tensor(beamlets).to(new_meta.device)\n",
    "        return new_meta\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "@dataclass\n",
    "class Metadata:\n",
    "    k_max : np.array \n",
    "    alpha_rad : float\n",
    "    E_ev : float \n",
    "    wavelength : float \n",
    "    rotation_deg : float \n",
    "    aberrations : np.array \n",
    "    sample_thickness_guess_angstrom : float\n",
    "    \n",
    "@dataclass\n",
    "class Metadata4D(Metadata):\n",
    "    scan_step : np.array \n",
    "    pixel_step : np.array \n",
    "    \n",
    "    def __init__(self, E_ev=None, ):\n",
    "        self.scan_step = np.array([0,0])\n",
    "        self.k_max = np.array([0,0])\n",
    "        self.alpha_rad = -1\n",
    "        self.rotation_deg = 0\n",
    "        self.E_ev = E_ev\n",
    "        self.wavelength = -1\n",
    "        self.aberrations = np.zeros((12,))\n",
    "        self.pixel_step = np.array([0,0])\n",
    "        self.sample_thickness_guess_angstrom = 0\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_dm4_file(filename):\n",
    "        m = Metadata4D()\n",
    "        with fileDM(filename) as f:\n",
    "            m.E_ev = f.allTags['.ImageList.2.ImageTags.Microscope Info.Voltage']\n",
    "            m.scan_step = np.array(f.scale[-2:]) * 10\n",
    "        m.wavelength = wavelength(m.E_ev)\n",
    "        return m \n",
    "    \n",
    "    def load_voltage_and_scanstep_from_dm4_(self, filename):\n",
    "        with fileDM(filename) as f:\n",
    "            self.E_ev = f.allTags['.ImageList.2.ImageTags.Microscope Info.Voltage']\n",
    "            self.scan_step = np.array(f.scale[-2:]) * 10\n",
    "        self.wavelength = wavelength(self.E_ev)\n",
    "        \n",
    "    def to_h5(self, file_path, key = 'meta'):\n",
    "        with h5py.File(file_path, 'a') as f:\n",
    "            g = f.create_group(key)\n",
    "            g.create_dataset('scan_step', data=self.scan_step)\n",
    "            g.create_dataset('k_max', data=self.k_max)\n",
    "            g.create_dataset('alpha_rad', data=self.alpha_rad)\n",
    "            g.create_dataset('rotation_deg', data=self.rotation_deg)\n",
    "            g.create_dataset('E_ev', data=self.E_ev)\n",
    "            g.create_dataset('wavelength', data=self.wavelength)\n",
    "            g.create_dataset('aberrations', data=self.aberrations)\n",
    "            g.create_dataset('pixel_step', data=self.pixel_step)\n",
    "            g.create_dataset('sample_thickness_guess_angstrom', data=self.sample_thickness_guess_angstrom)\n",
    "            \n",
    "    @staticmethod        \n",
    "    def from_h5(file_path, key = 'meta'):\n",
    "        res = Metadata4D()\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            g = f[key]\n",
    "            res.scan_step = g['scan_step'][...]\n",
    "            res.pixel_step = g['pixel_step'][...]\n",
    "            res.k_max = g['k_max'][...]\n",
    "            res.alpha_rad = g['alpha_rad'][()]\n",
    "            res.rotation_deg = g['rotation_deg'][()]\n",
    "            res.E_ev = g['E_ev'][()]\n",
    "            res.wavelength = g['wavelength'][()]\n",
    "            res.aberrations = g['aberrations'][...]\n",
    "            try:\n",
    "                res.sample_thickness_guess_angstrom = g['sample_thickness_guess_angstrom'][()]\n",
    "            except:\n",
    "                pass\n",
    "        return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "@dataclass\n",
    "class Sparse3DData:\n",
    "    __repr__ = basic_repr(['scan_dimensions', 'frame_dimensions'])\n",
    "    def __init__(self, indices = None, counts = None, scan_dimensions = None, frame_dimensions = None):\n",
    "        self.indices = indices\n",
    "        self.counts = counts\n",
    "        self.scan_dimensions = scan_dimensions \n",
    "        self.frame_dimensions = frame_dimensions \n",
    "        \n",
    "    def __getitem__(self, slice):\n",
    "        new_inds = self.indices[slice]\n",
    "        new_cts = self.counts[slice]\n",
    "        return Sparse3DData(new_inds, new_cts, (new_inds.shape[0],), self.frame_dimensions)\n",
    "    \n",
    "    def to(self, device, non_blocking = False):\n",
    "        self.indices = self.indices.to(device, non_blocking=non_blocking)\n",
    "        self.counts = self.counts.to(device, non_blocking=non_blocking)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_CoM(ar):\n",
    "    \"\"\"\n",
    "    Finds and returns the center of mass of array ar.\n",
    "    \"\"\"\n",
    "    nx, ny = np.shape(ar)\n",
    "    ry, rx = np.meshgrid(np.arange(ny), np.arange(nx))\n",
    "    tot_intens = np.sum(ar)\n",
    "    xCoM = np.sum(rx * ar) / tot_intens\n",
    "    yCoM = np.sum(ry * ar) / tot_intens\n",
    "    return xCoM, yCoM\n",
    "\n",
    "def get_probe_size(DP, thresh_lower=0.01, thresh_upper=0.99, N=100):\n",
    "    \"\"\"\n",
    "    Gets the center and radius of the probe in the diffraction plane.\n",
    "\n",
    "    The algorithm is as follows:\n",
    "    First, create a series of N binary masks, by thresholding the diffraction pattern DP with a\n",
    "    linspace of N thresholds from thresh_lower to thresh_upper, measured relative to the maximum\n",
    "    intensity in DP.\n",
    "    Using the area of each binary mask, calculate the radius r of a circular probe.\n",
    "    Because the central disk is typically very intense relative to the rest of the DP, r should\n",
    "    change very little over a wide range of intermediate values of the threshold. The range in which\n",
    "    r is trustworthy is found by taking the derivative of r(thresh) and finding identifying where it\n",
    "    is small.  The radius is taken to be the mean of these r values.\n",
    "    Using the threshold corresponding to this r, a mask is created and the CoM of the DP times this\n",
    "    mask it taken.  This is taken to be the origin x0,y0.\n",
    "\n",
    "    Accepts:\n",
    "        DP              (2D array) the diffraction pattern in which to find the central disk.\n",
    "                        A position averaged, or shift-corrected and averaged, DP work well.\n",
    "        thresh_lower    (float, 0 to 1) the lower limit of threshold values\n",
    "        thresh_upper    (float, 0 to 1) the upper limit of threshold values\n",
    "        N               (int) the number of thresholds / masks to use\n",
    "\n",
    "    Returns:\n",
    "        r               (float) the central disk radius, in pixels\n",
    "        x0              (float) the x position of the central disk center\n",
    "        y0              (float) the y position of the central disk center\n",
    "    \"\"\"\n",
    "    thresh_vals = np.linspace(thresh_lower,thresh_upper,N)\n",
    "    r_vals = np.zeros(N)\n",
    "\n",
    "    # Get r for each mask\n",
    "    DPmax = np.max(DP)\n",
    "    for i in range(len(thresh_vals)):\n",
    "        thresh = thresh_vals[i]\n",
    "        mask = DP > DPmax*thresh\n",
    "        r_vals[i] = np.sqrt(np.sum(mask)/np.pi)\n",
    "\n",
    "    # Get derivative and determine trustworthy r-values\n",
    "    dr_dtheta = np.gradient(r_vals)\n",
    "    mask = (dr_dtheta <= 0) * (dr_dtheta >= 2*np.median(dr_dtheta))\n",
    "    r = np.mean(r_vals[mask])\n",
    "\n",
    "    # Get origin\n",
    "    thresh = np.mean(thresh_vals[mask])\n",
    "    mask = DP > DPmax*thresh\n",
    "    x0,y0 = get_CoM(DP*mask)\n",
    "\n",
    "    return r,x0,y0\n",
    "\n",
    "class Sparse4DData:\n",
    "    __repr__ = basic_repr(['scan_dimensions', 'frame_dimensions'])\n",
    "    def __init__(self, indices = None, counts = None, scan_dimensions = None, frame_dimensions = None):\n",
    "        self.indices = indices\n",
    "        self.counts = counts\n",
    "        self.scan_dimensions = scan_dimensions \n",
    "        self.frame_dimensions = frame_dimensions \n",
    "        \n",
    "    def to_3d(self):\n",
    "        ish = self.indices.shape\n",
    "        _3d_view = (ish[0]*ish[1],ish[2])\n",
    "        return Sparse3DData(th.as_tensor(self.indices).view(*_3d_view), \n",
    "                            th.as_tensor(self.counts).view(*_3d_view), \n",
    "                            (_3d_view[0],), \n",
    "                            self.frame_dimensions)\n",
    "    @staticmethod\n",
    "    def stack_vertical(data0, data1):\n",
    "        no_count_indicator = np.iinfo(data0.indices.dtype).max\n",
    "        inds_max = np.max([data0.indices.shape[-1], data1.indices.shape[-1]])\n",
    "        inds_combined = np.zeros((data0.indices.shape[0] + data1.indices.shape[0], data0.indices.shape[1], inds_max),\n",
    "                                 dtype=data0.indices.dtype)\n",
    "        inds_combined[:] = no_count_indicator\n",
    "        counts_combined = np.zeros((data0.counts.shape[0] + data1.counts.shape[0], data0.counts.shape[1], inds_max),\n",
    "                                   dtype=data0.counts.dtype)\n",
    "        inds_combined[:data0.indices.shape[0], :, :data0.indices.shape[2]] = data0.indices\n",
    "        inds_combined[data0.indices.shape[0]:, :, :data1.indices.shape[2]] = data1.indices\n",
    "        \n",
    "        counts_combined[:data0.counts.shape[0], :, :data0.counts.shape[2]] = data0.counts\n",
    "        counts_combined[data0.counts.shape[0]:, :, :data1.counts.shape[2]] = data1.counts\n",
    "        \n",
    "        inds_slice = inds_combined\n",
    "        counts_slice = counts_combined\n",
    "        \n",
    "        data = Sparse4DData()\n",
    "        data.counts = counts_slice\n",
    "        data.indices = inds_slice\n",
    "        data.frame_dimensions = data0.frame_dimensions.copy()\n",
    "        data.scan_dimensions = np.array(data.counts.shape[:2])\n",
    "        \n",
    "        return data \n",
    "        \n",
    "    @staticmethod\n",
    "    def from_4Dcamera_file(filename, slice=None):\n",
    "        with h5py.File(filename, 'r') as f0:\n",
    "            frames = f0['electron_events/frames'][:]\n",
    "            scan_dimensions = (f0['electron_events/scan_positions'].attrs['Ny'],\n",
    "                               f0['electron_events/scan_positions'].attrs['Nx'])\n",
    "            frame_dimensions = np.array((576, 576))\n",
    "        \n",
    "        def unragged_frames_size(frames):\n",
    "            mm = 0\n",
    "            for ev in frames:\n",
    "                if ev.shape[0] > mm:\n",
    "                    mm = ev.shape[0]\n",
    "            return mm\n",
    "        \n",
    "        def make_unragged_frames(frames, scan_dimensions):\n",
    "            unragged_frame_size = unragged_frames_size(frames.ravel())\n",
    "            fr_full = np.zeros((frames.ravel().shape[0], unragged_frame_size), dtype=np.int32)\n",
    "            fr_full[:] = np.iinfo(fr_full.dtype).max\n",
    "            for ii, ev in enumerate(frames.ravel()):\n",
    "                fr_full[ii, :ev.shape[0]] = ev\n",
    "            fr_full_4d = fr_full.reshape((*scan_dimensions, fr_full.shape[1]))\n",
    "            fr_full_4d = fr_full_4d[:, :-1, :] \n",
    "            return fr_full_4d\n",
    "        \n",
    "        d = Sparse4DData()\n",
    "        d.indices = np.ascontiguousarray(make_unragged_frames(frames.ravel(), scan_dimensions))\n",
    "        if slice is not None:            \n",
    "            d.indices = d.indices[slice]\n",
    "        d.scan_dimensions = np.array(d.indices.shape[:2])\n",
    "        d.frame_dimensions = frame_dimensions\n",
    "        d.counts = np.zeros(d.indices.shape, dtype=np.bool)\n",
    "        d.counts[d.indices != np.iinfo(d.indices.dtype).max] = 1\n",
    "        \n",
    "        return d \n",
    "    \n",
    "    def crop_symmetric_center_(self, center, max_radius = None):\n",
    "        if max_radius is None:\n",
    "            y_min_radius = np.min([center[0], self.frame_dimensions[0] - center[0]])\n",
    "            x_min_radius = np.min([center[1], self.frame_dimensions[1] - center[1]])\n",
    "            max_radius = np.min([y_min_radius, x_min_radius])\n",
    "        inds = sp.to_device(self.indices,0)\n",
    "        frame_dimensions = sp.to_device(self.frame_dimensions, 0)\n",
    "        xp = sp.backend.get_array_module(inds)\n",
    "        new_frames, new_frame_dimensions = crop_symmetric_around_center(inds,frame_dimensions, center, max_radius)\n",
    "        print(f'old frames shape: {self.indices.shape}')\n",
    "        print(f'new frames shape: {new_frames.shape}')\n",
    "        print(f'old frames frame_dimensions: {self.frame_dimensions}')\n",
    "        print(f'new frames frame_dimensions: {new_frame_dimensions}')\n",
    "        self.indices = new_frames\n",
    "        self.counts = np.zeros(self.indices.shape, dtype=np.bool)\n",
    "        self.counts[self.indices != np.iinfo(self.indices.dtype).max] = 1\n",
    "        self.frame_dimensions = new_frame_dimensions\n",
    "        \n",
    "    def crop_symmetric_center(self, center, max_radius = None, verbose = True):\n",
    "        if max_radius is None:\n",
    "            y_min_radius = np.min([center[0], self.frame_dimensions[0] - center[0]])\n",
    "            x_min_radius = np.min([center[1], self.frame_dimensions[1] - center[1]])\n",
    "            max_radius = np.min([y_min_radius, x_min_radius])\n",
    "            \n",
    "        inds = sp.to_device(self.indices,0)\n",
    "        xp = sp.backend.get_array_module(inds)\n",
    "        frame_dims = xp.array(self.frame_dimensions)\n",
    "        new_frames, new_frame_dimensions = crop_symmetric_around_center(inds, frame_dims, center, max_radius)\n",
    "        \n",
    "        if verbose: \n",
    "            print(f'old frames shape: {self.indices.shape}')\n",
    "            print(f'new frames shape: {new_frames.shape}')\n",
    "            print(f'old frames frame_dimensions: {self.frame_dimensions}')\n",
    "            print(f'new frames frame_dimensions: {new_frame_dimensions}')\n",
    "            \n",
    "        res = Sparse4DData()\n",
    "        res.indices = new_frames\n",
    "        res.counts = np.zeros(self.indices.shape, dtype=np.bool)\n",
    "        res.counts[self.indices != np.iinfo(self.indices.dtype).max] = 1\n",
    "        res.frame_dimensions = new_frame_dimensions\n",
    "        res.scan_dimensions = self.scan_dimensions.copy()\n",
    "        return res\n",
    "    \n",
    "    def rotate_(self, angle_rad, center=None):\n",
    "        if center is None:\n",
    "            center = self.frame_dimensions // 2\n",
    "        new_indices = rotate(self.indices, self.frame_dimensions, center, angle_rad)\n",
    "        self.indices = new_indices\n",
    "    \n",
    "    def rotate(self, angle_rad, center=None):\n",
    "        if center is None:\n",
    "            center = self.frame_dimensions // 2\n",
    "        new_indices = rotate(self.indices, self.frame_dimensions, center, angle_rad)\n",
    "        res = Sparse4DData()\n",
    "        res.indices = new_indices\n",
    "        res.counts = self.counts.copy()\n",
    "        res.frame_dimensions = self.frame_dimensions\n",
    "        res.scan_dimensions = self.scan_dimensions.copy()\n",
    "        return res \n",
    "    \n",
    "    def sum_diffraction(self):\n",
    "        res = sum_frames(self.indices, self.counts, self.frame_dimensions)\n",
    "        return res \n",
    "    \n",
    "    @staticmethod\n",
    "    def _determine_center_and_radius(data : Sparse4DData, manual=False, size=25, threshold=3e-1):\n",
    "        sh = np.concatenate([data.scan_dimensions,data.frame_dimensions])\n",
    "        c = np.zeros((2,))\n",
    "        c[:] = (sh[-1] // 2, sh[-2] // 2)\n",
    "        radius = np.ones((1,)) * sh[-1] // 2  \n",
    "        inds = sp.to_device(data.indices[:size, :size].astype(np.uint32),0)\n",
    "        cts = sp.to_device(data.counts[:size, :size].astype(np.uint32),0)\n",
    "        xp = sp.backend.get_array_module(inds)\n",
    "        dc_subset = sparse_to_dense_datacube_crop(inds,cts, (size,size), data.frame_dimensions, c, radius, bin=2)\n",
    "        dcs = xp.sum(dc_subset, (0, 1))\n",
    "        m1 = dcs.get()\n",
    "        m = (gaussian(m1.astype(np.float32),2) > m1.max() * threshold).astype(np.float)\n",
    "        r, y0, x0 = get_probe_size(m)\n",
    "        return 2 * np.array([y0,x0]), r*2\n",
    "    \n",
    "    def determine_center_and_radius(self, manual=False, size=25, threshold=3e-1):\n",
    "        return Sparse4DData._determine_center_and_radius(self, manual, size=size, threshold=threshold)\n",
    "    \n",
    "    def to_dense(self, bin_factor, n_batches=4):\n",
    "        dense = sparse_to_dense_datacube_crop_gain_mask(self.indices, self.counts.astype(np.int16), self.scan_dimensions,\n",
    "                                                self.frame_dimensions, self.frame_dimensions/2, self.frame_dimensions[0]/2,\n",
    "                                                self.frame_dimensions[0]/2, binning=bin_factor, n_batches=n_batches, fftshift=False)\n",
    "        return dense\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_dense(dense, make_float = False, n_batches=1):\n",
    "        res = Sparse4DData()\n",
    "        res.frame_dimensions = np.array(dense.shape[-2:])\n",
    "        res.scan_dimensions = np.array(dense.shape[:2])\n",
    "        \n",
    "        cuda.select_device(1)\n",
    "        dev = th.device('cuda:1')\n",
    "\n",
    "        inds = np.prod(res.frame_dimensions)\n",
    "        if inds > 2**31:\n",
    "            dtype = th.int64\n",
    "        elif inds > 2**15:\n",
    "            dtype = th.int32\n",
    "        elif inds > 2**8:\n",
    "            dtype = th.int16\n",
    "        else:\n",
    "            dtype = th.uint8\n",
    "        \n",
    "        nonzeros = np.sum((dense > 0),(2,3))\n",
    "        nonzeros = np.max(nonzeros)\n",
    "\n",
    "        bits_counts = np.log2(dense.max())\n",
    "        if make_float:\n",
    "            dtype_counts = th.float32\n",
    "        else:\n",
    "            if bits_counts > np.log2(2**31-1):\n",
    "                dtype_counts = th.int64\n",
    "            elif bits_counts > np.log2(2**15-1):\n",
    "                dtype_counts = th.int32\n",
    "            elif bits_counts > 8:\n",
    "                dtype_counts = th.int16\n",
    "            else:\n",
    "                dtype_counts = th.uint8\n",
    "                \n",
    "        print(f'Using dtype: {dtype} for indices')\n",
    "        print(f'Using dtype: {dtype_counts} for counts')\n",
    "\n",
    "        threadsperblock = (16, 16)\n",
    "        blockspergrid = tuple(np.ceil(res.scan_dimensions / threadsperblock).astype(np.int32))\n",
    "        \n",
    "        ds = dense.shape\n",
    "        K = dense.shape[0]\n",
    "        divpts = array_split_divpoints_ntotal(K, n_batches)\n",
    "    \n",
    "        dense = th.as_tensor(dense) \n",
    "        indices = th.zeros((*dense.shape[:2], nonzeros), dtype=dtype)\n",
    "        counts = th.zeros((*dense.shape[:2], nonzeros), dtype=dtype_counts)\n",
    "    \n",
    "        indices[:] = th.iinfo(dtype).max\n",
    "        fd = th.tensor(res.frame_dimensions).to(dev)\n",
    "        for b in range(n_batches):\n",
    "        \n",
    "            denseb = dense[divpts[b]:divpts[b + 1]].to(dev)\n",
    "            indicesb = indices[divpts[b]:divpts[b + 1]].to(dev)\n",
    "            indicesb[:] = th.iinfo(dtype).max\n",
    "            countsb = counts[divpts[b]:divpts[b + 1]].to(dev)\n",
    "            \n",
    "            dense_to_sparse_kernel[blockspergrid, threadsperblock](denseb, indicesb, countsb, fd)\n",
    "            \n",
    "            counts[divpts[b]:divpts[b + 1]] = countsb.cpu()\n",
    "            indices[divpts[b]:divpts[b + 1]] = indicesb.cpu()\n",
    "\n",
    "        res.indices = indices.numpy()\n",
    "        res.counts = counts.numpy()\n",
    "        \n",
    "        print(f'frame_dimensions: {res.frame_dimensions}')\n",
    "        print(f'scan_dimensions : {res.scan_dimensions}')\n",
    "        \n",
    "        cuda.select_device(0)\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def fill_missing_data_(self):\n",
    "        data = self\n",
    "        import sigpy as sp\n",
    "        import cupy as cp\n",
    "        from numba import cuda\n",
    "    \n",
    "        empty_diffraction = np.sum(data.counts, 2) == 0\n",
    "        if empty_diffraction.sum() > 0:\n",
    "            empty_diffraction_indices = np.array(np.nonzero(empty_diffraction)).T\n",
    "            neighbors = np.stack([\n",
    "                empty_diffraction_indices + [1, 0],\n",
    "                empty_diffraction_indices + [1, 1],\n",
    "                empty_diffraction_indices + [0, 1],\n",
    "                empty_diffraction_indices + [-1, 0],\n",
    "                empty_diffraction_indices + [-1, -1],\n",
    "                empty_diffraction_indices + [0, -1],\n",
    "                empty_diffraction_indices + [1, -1],\n",
    "                empty_diffraction_indices + [-1, 1]\n",
    "            ])\n",
    "            eds = neighbors.shape\n",
    "            nb = neighbors.reshape((eds[0] * eds[1], 2))\n",
    "            nb[nb[:, 0] >= data.frame_dimensions[0], 0] = data.frame_dimensions[0] - 1\n",
    "            nb[nb[:, 1] >= data.frame_dimensions[1], 1] = data.frame_dimensions[1] - 1\n",
    "            nb[nb[:, 0] < 0, 0] = 0\n",
    "            nb[nb[:, 1] < 0, 1] = 0\n",
    "            cts_neighbors = data.counts[nb[:, 0], nb[:, 1]]\n",
    "            idx_neighbors = data.indices[nb[:, 0], nb[:, 1]]\n",
    "        \n",
    "            @cuda.jit\n",
    "            def sparse_to_dense_datacube_kernel_crop(dc, indices, counts, frame_dimensions, no_count_indicator):\n",
    "                n = cuda.grid(1)\n",
    "                N, MYBIN, MXBIN = dc.shape\n",
    "                MY, MX = frame_dimensions\n",
    "                if n < N:\n",
    "                    for i in range(indices[n].shape[0]):\n",
    "                        idx1d = indices[n, i]\n",
    "                        my = int(idx1d // MX)\n",
    "                        mx = int(idx1d - my * MX)\n",
    "                        n = int(1.0 * n)\n",
    "                        if idx1d != no_count_indicator:\n",
    "                            cuda.atomic.add(dc, (n, my, mx), counts[n, i])\n",
    "        \n",
    "        \n",
    "            def sparse_to_dense_datacube_crop(indices, counts, frame_dimensions):\n",
    "                xp = sp.backend.get_array_module(indices)\n",
    "                dc = xp.zeros((indices.shape[0], *frame_dimensions), dtype=counts.dtype)\n",
    "                print(dc.shape)\n",
    "                threadsperblock = (128,)\n",
    "                blockspergrid = tuple(np.ceil(np.array(indices.shape[0]) / threadsperblock).astype(np.int32))\n",
    "                no_count_indicator = np.iinfo(indices.dtype).max\n",
    "                sparse_to_dense_datacube_kernel_crop[blockspergrid, threadsperblock](dc, indices, counts,\n",
    "                                                                                     xp.array(frame_dimensions),\n",
    "                                                                                     no_count_indicator)\n",
    "                return dc\n",
    "        \n",
    "        \n",
    "            @cuda.jit\n",
    "            def dense_to_sparse_kernel(dense, indices, counts, frame_dimensions):\n",
    "                n = cuda.grid(1)\n",
    "                N, MYBIN, MXBIN = dense.shape\n",
    "                MY, MX = frame_dimensions\n",
    "                if n < N:\n",
    "                    k = 0\n",
    "                    for mx in range(MX):\n",
    "                        for my in range(MY):\n",
    "                            idx1d = my * MX + mx\n",
    "                            if dense[n, my, mx] > 0:\n",
    "                                indices[n, k] = idx1d\n",
    "                                counts[n, k] = dense[n, my, mx]\n",
    "                                k += 1\n",
    "        \n",
    "            dense = sparse_to_dense_datacube_crop(cp.array(idx_neighbors), cp.array(cts_neighbors).astype(np.float32),\n",
    "                                                  data.frame_dimensions)\n",
    "            dense = dense.reshape((eds[0], eds[1], *data.frame_dimensions))\n",
    "            replace = cp.round_(cp.mean(dense, 0)).astype(cp.uint8)\n",
    "        \n",
    "            indices_replace = cp.zeros((eds[1], data.counts.shape[2]), dtype=data.indices.dtype)\n",
    "            indices_replace[:] = np.iinfo(data.indices.dtype).max\n",
    "            counts_replace = cp.zeros((eds[1], data.counts.shape[2]), dtype=data.counts.dtype)\n",
    "            threadsperblock = (128,)\n",
    "            blockspergrid = tuple(np.ceil(np.array(eds[1]) / threadsperblock).astype(np.int32))\n",
    "            dense_to_sparse_kernel[blockspergrid, threadsperblock](replace, indices_replace, counts_replace,\n",
    "                                                                   cp.array(data.frame_dimensions))\n",
    "            replace_inds = empty_diffraction_indices.T\n",
    "            data.counts[replace_inds[0], replace_inds[1], :] = counts_replace.get()\n",
    "            data.indices[replace_inds[0], replace_inds[1], :] = indices_replace.get()\n",
    "    \n",
    "    @staticmethod\n",
    "    def rebin(sparse_data: Sparse4DData, bin_factor : int, n_batches=4) -> Sparse4DData:\n",
    "        dense = sparse_to_dense_datacube_crop_gain_mask(sparse_data.indices, sparse_data.counts.astype(np.int16), sparse_data.scan_dimensions, \n",
    "                                                sparse_data.frame_dimensions, sparse_data.frame_dimensions/2, sparse_data.frame_dimensions[0]/2,\n",
    "                                                sparse_data.frame_dimensions[0]/2, binning=bin_factor, n_batches=n_batches, fftshift=False)\n",
    "        sparse = Sparse4DData.from_dense(dense)\n",
    "        return sparse\n",
    "    \n",
    "    @staticmethod\n",
    "    def fftshift(sparse_data: Sparse4DData) -> Sparse4DData:\n",
    "        indices = sparse_data.indices\n",
    "        scan_dimensions = sparse_data.scan_dimensions\n",
    "        frame_dimensions = sparse_data.frame_dimensions\n",
    "        center_frame = frame_dimensions / 2\n",
    "        \n",
    "        threadsperblock = (16, 16)\n",
    "        blockspergrid = tuple(np.ceil(np.array(indices.shape[:2]) / threadsperblock).astype(np.int32))\n",
    "    \n",
    "        no_count_indicator = np.iinfo(indices.dtype).max\n",
    "        \n",
    "        inds = sp.to_device(indices, 0)\n",
    "        center_frame = sp.to_device(center_frame, 0)\n",
    "        scan_dimensions = sp.to_device(scan_dimensions, 0)\n",
    "        \n",
    "        fftshift_kernel[blockspergrid, threadsperblock](inds, center_frame, scan_dimensions, no_count_indicator)\n",
    "        \n",
    "        res = Sparse4DData()\n",
    "        res.indices = inds.get()\n",
    "        res.counts = sparse_data.counts.copy()\n",
    "        res.scan_dimensions = sparse_data.scan_dimensions.copy()\n",
    "        res.frame_dimensions = sparse_data.frame_dimensions.copy()\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    @staticmethod\n",
    "    def fftshift_and_pad_to(sparse_data: Sparse4DData, pad_to_frame_dimensions) -> Sparse4DData:\n",
    "        indices = sparse_data.indices\n",
    "        scan_dimensions = sparse_data.scan_dimensions\n",
    "        frame_dimensions = sparse_data.frame_dimensions\n",
    "        center_frame = frame_dimensions / 2\n",
    "\n",
    "        threadsperblock = (16, 16)\n",
    "        blockspergrid = tuple(np.ceil(np.array(indices.shape[:2]) / threadsperblock).astype(np.int32))\n",
    "\n",
    "        no_count_indicator_old = np.iinfo(indices.dtype).max\n",
    "        center_frame = sp.to_device(center_frame,0)\n",
    "        xp = sp.backend.get_array_module(center_frame)\n",
    "\n",
    "        inds = np.prod(pad_to_frame_dimensions)\n",
    "        if inds > 2**15:\n",
    "            dtype = xp.int64\n",
    "        elif inds > 2**15:\n",
    "            dtype = xp.int32\n",
    "        elif inds > 2**8:\n",
    "            dtype = xp.int16\n",
    "        else:\n",
    "            dtype = xp.uint8\n",
    "\n",
    "        no_count_indicator_new = xp.iinfo(dtype).max\n",
    "\n",
    "        inds = sp.to_device(indices,0).astype(dtype)\n",
    "        pad_to_frame_dimensions = sp.to_device(pad_to_frame_dimensions,0)\n",
    "        \n",
    "        scan_dimensions = sp.to_device(scan_dimensions,0)\n",
    "        \n",
    "        fftshift_pad_kernel[blockspergrid, threadsperblock](inds, center_frame, scan_dimensions,  pad_to_frame_dimensions, \n",
    "                                                            no_count_indicator_old, no_count_indicator_new)\n",
    "        sparse_data.indices = inds.get()\n",
    "        sparse_data.frame_dimensions = pad_to_frame_dimensions.get()\n",
    "        return sparse_data\n",
    "    \n",
    "    def fftshift_(self):\n",
    "        indices = self.indices\n",
    "        scan_dimensions = self.scan_dimensions\n",
    "        frame_dimensions = self.frame_dimensions\n",
    "        center_frame = frame_dimensions / 2\n",
    "        \n",
    "        threadsperblock = (16, 16)\n",
    "        blockspergrid = tuple(np.ceil(np.array(indices.shape[:2]) / threadsperblock).astype(np.int32))\n",
    "    \n",
    "        no_count_indicator = np.iinfo(indices.dtype).max\n",
    "        \n",
    "        inds = sp.to_device(indices, 0)\n",
    "        center_frame = sp.to_device(center_frame, 0)\n",
    "        scan_dimensions = sp.to_device(scan_dimensions, 0)\n",
    "        \n",
    "        fftshift_kernel[blockspergrid, threadsperblock](inds, center_frame, scan_dimensions, no_count_indicator)\n",
    "        self.indices = inds.get()\n",
    "        return self\n",
    "    \n",
    "    def fftshift_and_pad_to_(self, pad_to_frame_dimensions):\n",
    "        return Sparse4DData.fftshift_and_pad_to(self, pad_to_frame_dimensions)\n",
    "    \n",
    "    def bin(self, binning_factor, n_batches=4):\n",
    "        res = Sparse4DData.rebin(self, binning_factor, n_batches=n_batches)                \n",
    "        return res \n",
    "    \n",
    "    def virtual_annular_image(self, inner_radius, outer_radius, center):\n",
    "        inds = sp.to_device(self.indices, 0)\n",
    "        xp = sp.backend.get_array_module(inds)\n",
    "        cts = xp.array(self.counts, dtype=xp.uint32)\n",
    "        ctr = xp.array(center)\n",
    "        frame_dims = xp.array(self.frame_dimensions)\n",
    "        img = xp.zeros(tuple(self.scan_dimensions), dtype=xp.uint32)\n",
    "        no_count_indicator = xp.iinfo(self.indices.dtype).max\n",
    "        threadsperblock = (16, 16)\n",
    "        blockspergrid = tuple(np.ceil(np.array(self.indices.shape[:2]) / threadsperblock).astype(np.int32))\n",
    "        virtual_annular_image_kernel[blockspergrid, threadsperblock](img, inds, cts, inner_radius, outer_radius, ctr, \n",
    "                                                                     frame_dims, no_count_indicator)\n",
    "        return img.get()\n",
    "    \n",
    "    def fluence(self, dr):\n",
    "        sum_electrons = self.counts.sum()\n",
    "        area = np.prod(self.scan_dimensions) * dr**2\n",
    "        return sum_electrons/area\n",
    "\n",
    "    def flux(self, dr, dwell_time):\n",
    "        fluence = self.fluence(dr)\n",
    "        flux = fluence / (np.prod(self.scan_dimensions) * dwell_time)\n",
    "        return flux \n",
    "    \n",
    "    def slice(self, slice):\n",
    "        res = Sparse4DData()\n",
    "        res.indices = np.ascontiguousarray(self.indices[slice]) \n",
    "        res.counts = np.ascontiguousarray(self.counts[slice])\n",
    "        res.scan_dimensions = np.array(res.counts.shape[:2])\n",
    "        res.frame_dimensions = self.frame_dimensions.copy()     \n",
    "        return res \n",
    "    \n",
    "    def center_of_mass(self: Sparse4DData):\n",
    "        qx, qy = np.meshgrid(np.arange(self.scan_dimensions[0]),np.arange(self.scan_dimensions[1]))    \n",
    "        comx = th.zeros(tuple(self.scan_dimensions), dtype=th.float32)\n",
    "        comy = th.zeros(tuple(self.scan_dimensions), dtype=th.float32)\n",
    "        \n",
    "        no_count_indicator = np.iinfo(self.indices.dtype).max\n",
    "        \n",
    "        mass = np.sum(self.counts,2)\n",
    "        \n",
    "        threadsperblock = (16, 16)\n",
    "        blockspergrid = tuple(np.ceil(np.array(self.indices.shape[:2]) / threadsperblock).astype(np.int32))\n",
    "        \n",
    "        qx = th.tensor(qx).to(th.float32)\n",
    "        qy = th.tensor(qy).to(th.float32)\n",
    "        center_of_mass_kernel[blockspergrid, threadsperblock](comx, comy, th.tensor(self.indices), th.tensor(self.counts.astype(np.float32)), \n",
    "                                                              th.tensor(self.frame_dimensions), no_count_indicator, qx, qy)\n",
    "        comy = comy.get()\n",
    "        comx = comx.get()\n",
    "        comx /= mass + 1e-6\n",
    "        comy /= mass + 1e-6\n",
    "        comy[comy==0] = np.mean(comy[comy!=0])\n",
    "        comx[comx==0] = np.mean(comx[comx!=0])\n",
    "        comx -= np.mean(comx)\n",
    "        comy -= np.mean(comy)\n",
    "        return comy, comx    \n",
    "    \n",
    "    def to_h5(self, file_path, key):\n",
    "        with h5py.File(file_path, 'a') as f:\n",
    "            g = f.create_group(key)\n",
    "            g.create_dataset('counts', data=self.counts, compression=\"gzip\", compression_opts=7)\n",
    "            g.create_dataset('indices', data=self.indices, compression=\"gzip\", compression_opts=7)\n",
    "            g.create_dataset('frame_dimensions', data=self.frame_dimensions)\n",
    "            g.create_dataset('scan_dimensions', data=self.scan_dimensions)\n",
    "    \n",
    "    @staticmethod        \n",
    "    def from_h5(file_path, key):\n",
    "        res = Sparse4DData()\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            g = f[key]\n",
    "            res.counts = g['counts'][...]\n",
    "            res.indices = g['indices'][...]\n",
    "            res.frame_dimensions = g['frame_dimensions'][...]\n",
    "            res.scan_dimensions = g['scan_dimensions'][...]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class Dense4DDataset(Dataset):\n",
    "    data : th.tensor\n",
    "    scan_dimensions : np.array\n",
    "    scan_positions : int\n",
    "    frame_dimensions : np.array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index) -> th.tensor:\n",
    "        return self.data[index]\n",
    "\n",
    "    def __init__(self, data: Union[np.array, th.tensor]):\n",
    "        dsh = data.shape\n",
    "        self.data = th.as_tensor(data).view(dsh[0]*dsh[1],dsh[2],dsh[3])\n",
    "        self.scan_positions = self.data.shape[0]\n",
    "        self.scan_dimensions = np.array(dsh[:2])\n",
    "        self.frame_dimensions = np.array(dsh[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export   \n",
    "\n",
    "class LinearIndexEncoded4DDataset(Dataset):\n",
    "    sparse_data : Sparse3DData\n",
    "    scan_dimensions : np.array\n",
    "    scan_positions : int\n",
    "    frame_dimensions : np.array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.sparse_data.scan_dimensions[0]\n",
    "\n",
    "    def __getitem__(self, index) -> Sparse3DData:\n",
    "        return self.sparse_data[index]\n",
    "\n",
    "    def __init__(self, sparse_data: Sparse4DData):\n",
    "        self.sparse_data = sparse_data.to_3d()\n",
    "        self.scan_positions = self.sparse_data.scan_dimensions\n",
    "        self.scan_dimensions = sparse_data.scan_dimensions\n",
    "        self.frame_dimensions = sparse_data.frame_dimensions\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
