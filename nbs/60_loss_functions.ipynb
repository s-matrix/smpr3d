{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "\n",
    "> API details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import numpy as np\n",
    "import torch as th\n",
    "import math as m\n",
    "import numba.cuda as cuda\n",
    "\n",
    "@cuda.jit\n",
    "def prox_D_gaussian_kernel(z, z_hat, a, beta, a_strides):\n",
    "    \"\"\"\n",
    "\n",
    "    :param z:           D x K x My x Mx x 2\n",
    "    :param z_hat:       D x K x My x Mx x 2\n",
    "    :param a:           D x K x My x Mx\n",
    "    :param beta:        1\n",
    "    :param a_strides:   (4,)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    n = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    D, K, MY, MX, _ = z.shape\n",
    "    N1 = D * K * MY * MX\n",
    "\n",
    "    d = n // a_strides[0]\n",
    "    k = (n - d * a_strides[0]) // a_strides[1]\n",
    "    my = (n - d * a_strides[0] - k * a_strides[1]) // a_strides[2]\n",
    "    mx = (n - d * a_strides[0] - k * a_strides[1] - my * a_strides[2]) // a_strides[3]\n",
    "\n",
    "    if n < N1:\n",
    "        z_hatc = z_hat[d, k, my, mx, 0] + 1j * z_hat[d, k, my, mx, 1]\n",
    "        abs_zhat_c = abs(z_hatc)\n",
    "        if abs_zhat_c != 0:\n",
    "            sgn_zhat = z_hatc / abs_zhat_c\n",
    "            fac = (a[d, k, my, mx] + beta * abs_zhat_c) / (1.0 + beta)\n",
    "            zc = fac * sgn_zhat\n",
    "            z[d, k, my, mx, 0] = zc.real\n",
    "            z[d, k, my, mx, 1] = zc.imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def prox_D_gaussian(z, z_hat, a, beta):\n",
    "    \"\"\"\n",
    "    Proximal operator of the Gaussian log-likelihood.\n",
    "\n",
    "    :param z:           D x K x My x Mx x 2, updated exit waves\n",
    "    :param z_hat:       D x K x My x Mx x 2, current model exit waves\n",
    "    :param a:           D x K x My x Mx,     measured amplitudes\n",
    "    :param beta:        float                hyperparameter\n",
    "\n",
    "    :return: z\n",
    "    \"\"\"\n",
    "    gpu = cuda.get_current_device()\n",
    "    stream = th.cuda.current_stream().cuda_stream\n",
    "    threadsperblock = gpu.MAX_THREADS_PER_BLOCK // 2\n",
    "    blockspergrid = m.ceil(np.prod(np.array(a.shape)) / threadsperblock)\n",
    "    strides = th.tensor(a.stride()).to(z.device)\n",
    "    prox_D_gaussian_kernel[blockspergrid, threadsperblock, stream](z, z_hat, a, beta, strides)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "@cuda.jit\n",
    "def gradz_poisson_sparse_kernel(out, z, a_indices, a_counts, no_count_indicator, total_cts):\n",
    "    \"\"\"\n",
    "\n",
    "    :param z:           D x K x My x Mx x 2\n",
    "    :param z_hat:       D x K x My x Mx x 2\n",
    "    :param a_indices:   D x K x counts\n",
    "    :param a_counts:    D x K x counts\n",
    "    :param beta:        float\n",
    "    :param frame_dimensions: (2,)\n",
    "    :param no_count_indicator: float or int\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    d, k = cuda.grid(2)\n",
    "    D, K, MY, MX, _ = z.shape\n",
    "    if d < D and k < K:\n",
    "        if total_cts[d,k] > 0:\n",
    "            for i in range(a_indices[d, k].shape[0]):\n",
    "                idx1d = a_indices[d, k, i]\n",
    "                if idx1d != no_count_indicator:\n",
    "                    my = idx1d // MX\n",
    "                    mx = idx1d - my * MX\n",
    "                    zc = z[d, k, my, mx, 0] + 1j * z[d, k, my, mx, 1]\n",
    "                    abs_zc = abs(zc)\n",
    "                    # if abs_zc != 0:\n",
    "                    fac = 1 - (a_counts[d, k, i] / (abs_zc**2+1e-2))\n",
    "                    # else:\n",
    "                    #     fac = 1 - (a_counts[d, k, i] / 1e-3)\n",
    "                    zc *= fac\n",
    "                    out[d, k, my, mx, 0] = zc.real\n",
    "                    out[d, k, my, mx, 1] = zc.imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def gradz_poisson_sparse(out, z, a_indices, a_counts):\n",
    "    \"\"\"\n",
    "    Proximal operator of the Gaussian log-likelihood. Sparse version\n",
    "\n",
    "    :param z:           D x K x My x Mx x 2, updated exit waves\n",
    "    :param z_hat:       D x K x My x Mx x 2, current model exit waves\n",
    "    :param a_indices:   D x K x cts,     measured amplitude indices\n",
    "    :param a_counts:    D x K x cts,     measured amplitude counts\n",
    "    :param beta:        float                hyperparameter\n",
    "\n",
    "    :return: z\n",
    "    \"\"\"\n",
    "    gpu = cuda.get_current_device()\n",
    "    stream = th.cuda.current_stream().cuda_stream\n",
    "    threadsperblock = (2, 32)\n",
    "    blockspergrid = tuple(np.ceil(np.array(z.shape[:2]) / threadsperblock).astype(np.int32))\n",
    "    no_count_indicator = th.iinfo(a_indices.dtype).max\n",
    "    total_cts = th.sum(a_counts,2)\n",
    "    gradz_poisson_sparse_kernel[blockspergrid, threadsperblock, stream](out, z, a_indices, a_counts, no_count_indicator,\n",
    "                                                                        total_cts)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "@cuda.jit\n",
    "def gradz_gaussian_sparse_kernel(out, z, a_indices, a_counts, no_count_indicator):\n",
    "    \"\"\"\n",
    "\n",
    "    :param z:           D x K x My x Mx x 2\n",
    "    :param z_hat:       D x K x My x Mx x 2\n",
    "    :param a_indices:   D x K x counts\n",
    "    :param a_counts:    D x K x counts\n",
    "    :param beta:        float\n",
    "    :param frame_dimensions: (2,)\n",
    "    :param no_count_indicator: float or int\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    d, k = cuda.grid(2)\n",
    "    D, K, MY, MX, _ = z.shape\n",
    "    if d < D and k < K:\n",
    "        for i in range(a_indices[d, k].shape[0]):\n",
    "            idx1d = a_indices[d, k, i]\n",
    "            if idx1d != no_count_indicator:\n",
    "                my = idx1d // MX\n",
    "                mx = idx1d - my * MX\n",
    "                zc = z[d, k, my, mx, 0] + 1j * z[d, k, my, mx, 1]\n",
    "                abs_zc = abs(zc)\n",
    "                if abs_zc != 0:\n",
    "                    fac = 1 - (float(a_counts[d, k, i]) / abs_zc)\n",
    "                else:\n",
    "                    fac = 1 - (float(a_counts[d, k, i]) / 1e-3)\n",
    "                zc *= fac\n",
    "                out[d, k, my, mx, 0] = zc.real\n",
    "                out[d, k, my, mx, 1] = zc.imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "def gradz_gaussian_sparse(out, z, a_indices, a_counts):\n",
    "    \"\"\"\n",
    "    Proximal operator of the Gaussian log-likelihood. Sparse version\n",
    "\n",
    "    :param z:           D x K x My x Mx x 2, updated exit waves\n",
    "    :param z_hat:       D x K x My x Mx x 2, current model exit waves\n",
    "    :param a_indices:   D x K x cts,     measured amplitude indices\n",
    "    :param a_counts:    D x K x cts,     measured amplitude counts\n",
    "    :param beta:        float                hyperparameter\n",
    "\n",
    "    :return: z\n",
    "    \"\"\"\n",
    "    gpu = cuda.get_current_device()\n",
    "    stream = th.cuda.current_stream().cuda_stream\n",
    "    threadsperblock = (2, 32)\n",
    "    blockspergrid = tuple(np.ceil(np.array(z.shape[:2]) / threadsperblock).astype(np.int32))\n",
    "    no_count_indicator = th.iinfo(a_indices.dtype).max\n",
    "    gradz_gaussian_sparse_kernel[blockspergrid, threadsperblock, stream](out, z, a_indices, a_counts, no_count_indicator)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "from numba import cuda\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def sparse_amplitude_loss_kernel(a_model, indices_target, counts_target, loss, grad, frame_dimensions,\n",
    "                                 no_count_indicator):\n",
    "    k = cuda.grid(1)\n",
    "    K, _ = indices_target.shape\n",
    "    MY, MX = frame_dimensions\n",
    "    if k < K:\n",
    "        for i in range(indices_target[k].shape[0]):\n",
    "            idx1d = indices_target[k, i]\n",
    "            my = idx1d // MX\n",
    "            mx = idx1d - my * MX\n",
    "            if idx1d != no_count_indicator:\n",
    "                grad[k, my, mx] = 1 - (counts_target[k, i] / (a_model[k, my, mx] + 1e-2))\n",
    "                cuda.atomic.add(loss, (0), (a_model[k, my, mx] - counts_target[k, i]) ** 2)\n",
    "                \n",
    "def sparse_amplitude_loss(a_model, indices_target, counts_target, frame_dimensions):\n",
    "    \"\"\"\n",
    "\n",
    "    :param a_model:             K x M1 x M2\n",
    "    :param indices_target:      K x num_max_counts\n",
    "    :param counts_target:       K x num_max_counts\n",
    "    :param frame_dimensions:    2\n",
    "    :return: loss (1,), grad (K x M1 x M2)\n",
    "    \"\"\"\n",
    "    threadsperblock = (256,)\n",
    "    blockspergrid = tuple(np.ceil(np.array(indices_target.shape[0]) / threadsperblock).astype(np.int32))\n",
    "\n",
    "    loss = th.zeros((1,), device=a_model.device, dtype=th.float32)\n",
    "    grad = th.ones_like(a_model)\n",
    "    no_count_indicator = th.iinfo(indices_target.dtype).max\n",
    "    sparse_amplitude_loss_kernel[blockspergrid, threadsperblock](a_model.detach(), indices_target.detach(),\n",
    "                                                                 counts_target.detach(), loss.detach(), grad.detach(),\n",
    "                                                                 frame_dimensions, no_count_indicator)\n",
    "    return loss, grad\n",
    "\n",
    "import math as m\n",
    "import cmath as cm\n",
    "@cuda.jit\n",
    "def sparse_smooth_truncated_amplitude_loss_kernel(a_model, indices_target, counts_target, loss, grad, no_count_indicator, eps):\n",
    "    n = cuda.grid(1)\n",
    "    K, MY, MX = a_model.shape\n",
    "    N = K * MY * MX\n",
    "\n",
    "    k = n // (MY * MX)\n",
    "    my = (n - k * (MY * MX)) // MX\n",
    "    mx = (n - k * (MY * MX) - my * MX)\n",
    "    \n",
    "    if n < N:\n",
    "        idx1d = mx + my * MX\n",
    "        a_measure = 0\n",
    "        a_model = a_model[k, my, mx]\n",
    "        for i in range(indices_target.shape[1]):\n",
    "            if indices_target[k, i] == idx1d and no_count_indicator != indices_target[k, i]:\n",
    "                a_measure = m.sqrt(counts_target[k, i])\n",
    "            \n",
    "        if a_model < eps * a_measure:\n",
    "            grad[k, my, mx] =  (1-1/eps) \n",
    "            loss_k = (1-eps)/2*(a_measure**2-(1/eps)*a_model**2)\n",
    "        else:\n",
    "            grad[k, my, mx] =  1 - (a_measure/a_model)\n",
    "            loss_k = .5 * abs(a_model - a_measure)**2\n",
    "        loss[k] = loss_k\n",
    "        # cuda.atomic.add(loss, (k), loss_k)\n",
    "            \n",
    "\n",
    "def sparse_smooth_truncated_amplitude_loss(a_model, indices_target, counts_target, frame_dimensions, eps=0.1):\n",
    "    \"\"\"\n",
    "    Smooth truncated amplitude loss from Chang et al., Overlapping Domain Decomposition Methods for Ptychographic Imaging, (2020)\n",
    "    \n",
    "    :param a_model:             K x M1 x M2\n",
    "    :param indices_target:      K x num_max_counts\n",
    "    :param counts_target:       K x num_max_counts\n",
    "    :param frame_dimensions:    2\n",
    "    :return: loss (K,), grad (K x M1 x M2)\n",
    "    \"\"\"\n",
    "    \n",
    "    threadsperblock = (256,)\n",
    "    blockspergrid = tuple(np.ceil(np.array(np.prod(a_model.shape)) / threadsperblock).astype(np.int32))\n",
    "\n",
    "    loss = th.zeros((a_model.shape[0],), device=a_model.device, dtype=th.float32)\n",
    "    grad = th.ones_like(a_model)\n",
    "    no_count_indicator = th.iinfo(indices_target.dtype).max\n",
    "    \n",
    "    sparse_smooth_truncated_amplitude_loss_kernel[blockspergrid, threadsperblock](a_model.detach(), indices_target.detach(),\n",
    "                                                                 counts_target.detach(), loss.detach(), grad.detach(),\n",
    "                                                                 no_count_indicator, eps)\n",
    "    return loss, grad\n",
    "\n",
    "@cuda.jit\n",
    "def sparse_smooth_truncated_amplitude_prox_kernel(a_model, indices_target, counts_target, loss, grad, no_count_indicator, eps, lam):\n",
    "    n = cuda.grid(1)\n",
    "    K, MY, MX = a_model.shape\n",
    "    N = K * MY * MX\n",
    "\n",
    "    k = n // (MY * MX)\n",
    "    my = (n - k * (MY * MX)) // MX\n",
    "    mx = (n - k * (MY * MX) - my * MX)\n",
    "    \n",
    "    if n < N:\n",
    "        idx1d = mx + my * MX\n",
    "        a_measure = 0\n",
    "        a_model = a_model[k, my, mx]\n",
    "        for i in range(indices_target.shape[1]):\n",
    "            if indices_target[k, i] == idx1d and no_count_indicator != indices_target[k, i]:\n",
    "                a_measure = m.sqrt(counts_target[k, i])\n",
    "            \n",
    "        # if a_model < eps * a_measure:\n",
    "        #     grad[k, my, mx] =  max(0, lam * a_measure/(lam-((1-eps)/eps)))/ a_model\n",
    "        #     loss_k = (1-eps)/2*(a_measure**2-(1/eps)*a_model**2)\n",
    "        # else:\n",
    "        #     grad[k, my, mx] =  ((a_measure+lam * a_model)/(1+lam))/ a_model\n",
    "        #     loss_k = .5 * abs(a_model - a_measure)**2\n",
    "            \n",
    "        grad[k, my, mx] =  a_measure/ a_model\n",
    "        loss_k = .5 * abs(a_model - a_measure)**2    \n",
    "        \n",
    "        loss[k] = loss_k\n",
    "            \n",
    "\n",
    "def sparse_smooth_truncated_amplitude_prox(a_model, indices_target, counts_target, frame_dimensions, eps=0.5, lam=6e-1):\n",
    "    \"\"\"\n",
    "    Smooth truncated amplitude loss from Chang et al., Overlapping Domain Decomposition Methods for Ptychographic Imaging, (2020)\n",
    "    \n",
    "    :param a_model:             K x M1 x M2\n",
    "    :param indices_target:      K x num_max_counts\n",
    "    :param counts_target:       K x num_max_counts\n",
    "    :param frame_dimensions:    2\n",
    "    :return: loss (K,), grad (K x M1 x M2)\n",
    "    \"\"\"\n",
    "    \n",
    "    threadsperblock = (256,)\n",
    "    blockspergrid = tuple(np.ceil(np.array(np.prod(a_model.shape)) / threadsperblock).astype(np.int32))\n",
    "\n",
    "    loss = th.zeros((a_model.shape[0],), device=a_model.device, dtype=th.float32)\n",
    "    grad = th.ones_like(a_model)\n",
    "    no_count_indicator = th.iinfo(indices_target.dtype).max\n",
    "    \n",
    "    sparse_smooth_truncated_amplitude_prox_kernel[blockspergrid, threadsperblock](a_model.detach(), indices_target.detach(),\n",
    "                                                                 counts_target.detach(), loss.detach(), grad.detach(),\n",
    "                                                                 no_count_indicator, eps, lam)\n",
    "    return loss, grad\n",
    "\n",
    "@cuda.jit\n",
    "def sparse_amplitude_prox_kernel(a_model, indices_target, counts_target, loss, grad, no_count_indicator, eps, lam):\n",
    "    n = cuda.grid(1)\n",
    "    K, MY, MX = a_model.shape\n",
    "    N = K * MY * MX\n",
    "\n",
    "    k = n // (MY * MX)\n",
    "    my = (n - k * (MY * MX)) // MX\n",
    "    mx = (n - k * (MY * MX) - my * MX)\n",
    "    \n",
    "    if n < N:\n",
    "        idx1d = mx + my * MX\n",
    "        a_measure = 0\n",
    "        a_model = a_model[k, my, mx]\n",
    "        for i in range(indices_target.shape[1]):\n",
    "            if indices_target[k, i] == idx1d and no_count_indicator != indices_target[k, i]:\n",
    "                a_measure = m.sqrt(counts_target[k, i])\n",
    "            \n",
    "        grad[k, my, mx] =  a_measure/ a_model\n",
    "        loss_k = .5 * abs(a_model - a_measure)**2    \n",
    "        \n",
    "        loss[k] = loss_k\n",
    "            \n",
    "\n",
    "def sparse_amplitude_prox(a_model, indices_target, counts_target, frame_dimensions, eps=0.5, lam=6e-1):\n",
    "    \"\"\"\n",
    "    Smooth truncated amplitude loss from Chang et al., Overlapping Domain Decomposition Methods for Ptychographic Imaging, (2020)\n",
    "    \n",
    "    :param a_model:             K x M1 x M2\n",
    "    :param indices_target:      K x num_max_counts\n",
    "    :param counts_target:       K x num_max_counts\n",
    "    :param frame_dimensions:    2\n",
    "    :return: loss (K,), grad (K x M1 x M2)\n",
    "    \"\"\"\n",
    "    \n",
    "    threadsperblock = (256,)\n",
    "    blockspergrid = tuple(np.ceil(np.array(np.prod(a_model.shape)) / threadsperblock).astype(np.int32))\n",
    "\n",
    "    loss = th.zeros((a_model.shape[0],), device=a_model.device, dtype=th.float32)\n",
    "    grad = th.ones_like(a_model)\n",
    "    no_count_indicator = th.iinfo(indices_target.dtype).max\n",
    "    \n",
    "    sparse_amplitude_prox_kernel[blockspergrid, threadsperblock](a_model.detach(), indices_target.detach(),\n",
    "                                                                 counts_target.detach(), loss.detach(), grad.detach(),\n",
    "                                                                 no_count_indicator, eps, lam)\n",
    "    return loss, grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
